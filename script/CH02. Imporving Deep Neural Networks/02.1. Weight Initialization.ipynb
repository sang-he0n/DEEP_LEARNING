{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.1. **Weight Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **가중치 초기화(Weight Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델 학습의 시작 단계에서 네트워크의 각 연결 가중치를 설정하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 레이어 별 노드가 모두 같은 가중치로 시작한다면 모든 뉴런이 동일 출력을 하게 되어, 노드 확장의 의미가 소실됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① 무작위 초기화(Random Initialization)\n",
    "##### $ \\hspace{0.15cm} $ ② 자비에 초기화(Xavier Initialization)\n",
    "##### $ \\hspace{0.15cm} $ ③ 히 초기화(He Initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **무작위 초기화(Random Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 학습 전 가중치를 단순히 무작위로 설정하는 초기화 방법\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\sigma{}^{2}) \\,\\, \\text{ or } \\,\\, W^{[l]} \\sim{} U(-a,a) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 가중치를 무작위로 설정하여 파라미터 초기값의 대칭성을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 초기 가중치가 너무 크거나 작을 경우 수렴하지 않거나, 학습이 매우 느려져 깊은 신경망에서는 학습 불안정성 초래"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **자비에 초기화(Xavier Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 네트워크 층의 입력과 출력 수 고려하여 초기화의 범위를 조정하는 초기화 방법\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}+n^{[l]}}) \\,\\, \\text{ or } \\,\\, W^{[l]} \\sim{} U(-\\sqrt{\\frac{6}{n^{[l-1]}+n^{[l]}}},\\sqrt{\\frac{6}{n^{[l-1]}+n^{[l]}}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** \n",
    "##### $ \\hspace{0.15cm} \\text{var}(z^{[l]}_{ij}) = \\text{var}() $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 입력 뉴런 수($ n^{[l-1]} $)및 출력 뉴런 수($ n^{[l]} $)를 모두 고려하여 출력(활성화)의 분산이 급격히 변하지 않도록 조절\n",
    "##### $ \\hspace{0.15cm} $ ② **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 깊은 신경망에서도 상대적으로 안정적이며 빠르게 수렴 가능\n",
    "##### $ \\hspace{0.15cm} $ ② 활성화 값과 기울기의 분산을 균형 있게 유지하여 학습 안정성 증가\n",
    "##### $ \\hspace{0.15cm} $ ③ 활성화 함수의 출력 대칭성을 가정하기에 시그모이드, 하이퍼볼릭 탄젠트 함수를 사용하는 층에서 효과적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 렐루 활성화 함수에서 성능 저하 가능성 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **히 초기화(He Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 렐루(ReLU) 종류의 활성화 함수를 사용하는 신경망에서 층 간 신호 분산을 일정하게 유지하도록 고안\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** \n",
    "##### $ \\hspace{0.15cm} \\text{var}(z^{[l]}_{ij}) = \\text{var}() $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 렐루 활성화 함수에서 가중치 값의 분산을 효과적으로 유지\n",
    "##### $ \\hspace{0.15cm} $ ② 렐루 계열 활성화 함수의 특성을 반영하여 음의 값이 제거되는 현상을 보정\n",
    "##### $ \\hspace{0.15cm} $ ③ 신경망의 각 층에서 활성화 분산이 크게 줄어들지 않도록 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 기울기 소실 문제를 크게 완화\n",
    "##### $ \\hspace{0.15cm} $ ② **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 시그모이드, 하이퍼볼릭 탄젠트 활성화 함수에서는 다소 성능 저하 가능성 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** **[CONTENTS]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
