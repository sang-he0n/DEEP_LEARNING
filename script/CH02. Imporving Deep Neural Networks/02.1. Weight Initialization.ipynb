{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.1. **Weight Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **가중치 초기화(Weight Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델 학습의 시작 단계에서 네트워크의 각 연결 가중치를 설정하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 레이어 별 노드가 모두 같은 가중치로 시작한다면 모든 노드가 동일 출력을 하게 되어, 노드 확장의 의미가 소실됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① 무작위 초기화(Random Initialization)\n",
    "##### $ \\hspace{0.15cm} $ ② 자비에 초기화(Xavier Initialization)\n",
    "##### $ \\hspace{0.15cm} $ ③ 히 초기화(He Initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **무작위 초기화(Random Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 학습 전 가중치를 단순히 무작위로 설정하는 초기화 방법\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\sigma{}^{2}) \\,\\, \\text{ or } \\,\\, W^{[l]} \\sim{} U(-a,a) \\;\\; \\text{ where } \\, a \\, \\text{ is } $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 가중치를 정규분포 혹은 균등분포를 따른다고 가정해 무작위로 설정하여 파라미터 초기값의 대칭성을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 낮은 계산 비용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 초기 가중치가 너무 크거나 작을 경우, 기울기가 소실되거나 폭발할 수 있어 학습 불안정성 초래\n",
    "##### $ \\hspace{0.15cm} $ ② 가중치 스케일에 대한 고려 미비\n",
    "##### $ \\hspace{0.15cm} $ ③ 각 층에 대한 입력과 출력의 분산을 고려하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 입력과 출력의 분산을 고려하지 않았을 때의 문제점 : \n",
    "##### $ \\hspace{0.15cm} \\text{if } \\, \\mathbb{E}[A^{[l-1](i)}_{k}] = 0, \\;\\; \\text{var}(A^{[l-1](i)}_{k}) = C < \\infty{} \\; \\text{ and } \\; W^{[l]}_{m,k} \\sim{} N(0, \\sigma{}^{2}). $\n",
    "##### $ \\hspace{0.3cm} Z^{[l](i)}_{m} = \\displaystyle\\sum^{n^{[l-1]}}_{k=1} W^{[l]}_{m,k}A^{[l](i)}_{k} + b^{[l]}_{m} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **자비에 초기화(Xavier Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 네트워크 층의 입력과 출력 수 고려하여 초기화의 범위를 조정하는 초기화 방법\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}+n^{[l]}}) \\,\\, \\text{ or } \\,\\, W^{[l]} \\sim{} U(-\\sqrt{\\frac{6}{n^{[l-1]}+n^{[l]}}},\\sqrt{\\frac{6}{n^{[l-1]}+n^{[l]}}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 입력 노드 수($ n^{[l-1]} $)및 출력 노드 수($ n^{[l]} $)를 모두 고려하여 출력(활성화)의 분산이 일정하도록 유도\n",
    "##### $ \\hspace{0.15cm} $ ② 활성화 함수의 출력 대칭성을 가정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 깊은 신경망에서도 (상대적으로) 안정적이며 빠르게 수렴 가능\n",
    "##### $ \\hspace{0.15cm} $ ② 활성화 값과 기울기의 분산을 균형 있게 유지하여 기울기 소실/폭발 **완화**하여 학습 안정성 증가\n",
    "##### $ \\hspace{0.15cm} $ ③ 시그모이드, 하이퍼볼릭 탄젠트 함수를 사용하는 레이어에서 효과적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 렐루 활성화 함수에서 성능 저하 가능성 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **히 초기화(He Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 렐루(ReLU) 종류의 활성화 함수를 사용하는 신경망에서 층 간 신호 분산을 일정하게 유지하도록 고안\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}}) \\,\\, \\text{ or } \\,\\, W^{[l]} \\sim{} U(-\\sqrt{\\frac{6}{n^{[l-1]}}},\\sqrt{\\frac{6}{n^{[l-1]}}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 렐루 활성화 함수에서 가중치 값의 분산을 효과적으로 유지\n",
    "##### $ \\hspace{0.15cm} $ ② 렐루 계열 활성화 함수의 특성을 반영하여 음의 값이 제거되는 현상을 보정\n",
    "##### $ \\hspace{0.15cm} $ ③ 신경망의 각 층에서 활성화 분산이 크게 줄어들지 않도록 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 기울기 소실 문제를 크게 완화\n",
    "##### $ \\hspace{0.15cm} $ ② 렐루 계열 활성화 함수를 사용하는 레이어에서 효과적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 시그모이드, 하이퍼볼릭 탄젠트 활성화 함수에서는 분산이 커 다소 성능 저하 가능성 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** **[CONTENTS]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
