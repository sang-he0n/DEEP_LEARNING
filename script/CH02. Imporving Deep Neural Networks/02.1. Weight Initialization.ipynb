{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.1. **Weight Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **가중치 초기화(Weight Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델 학습의 시작 단계에서 네트워크의 각 연결 가중치를 설정하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 레이어 별 노드가 모두 같은 가중치로 시작한다면 모든 노드가 동일 출력을 하게 되어, 노드 확장의 의미가 소실됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① 무작위 초기화(Random Initialization)\n",
    "##### $ \\hspace{0.15cm} $ ② 자비에 초기화(Xavier Initialization)\n",
    "##### $ \\hspace{0.15cm} $ ③ 히 초기화(He Initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **무작위 초기화(Random Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 학습 전 가중치를 단순히 무작위로 설정하는 초기화 방법\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\sigma{}^{2}) \\,\\, \\text{ or } \\,\\, W^{[l]} \\sim{} U(-a,a) \\;\\; \\text{ where } \\; a \\, \\text{ is scale parameter}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 가중치를 정규분포 혹은 균등분포를 따른다고 가정해 무작위로 설정하여 파라미터 초기값의 대칭성을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 낮은 계산 비용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 초기 가중치가 너무 크거나 작을 경우, 기울기가 소실되거나 폭발할 수 있어 학습 불안정성 초래\n",
    "##### $ \\hspace{0.15cm} $ ② 가중치 스케일에 대한 고려 미비\n",
    "##### $ \\hspace{0.15cm} $ ③ 각 층에 대한 입력과 출력의 분산을 고려하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 순전파에서 입력($ a^{[l-1](i)}_{p} $)과 출력($ z^{[l](i)}_{p} $)의 분산을 고려하지 않았을 때의 문제점 : \n",
    "##### $ \\hspace{0.15cm} \\text{if } \\; \\mathbb{E}[a^{[l-1](i)}_{p}] = 0, \\;\\; \\text{var}[a^{[l-1](i)}_{p}] = c^{2} < \\infty{} $\n",
    "##### $ \\hspace{0.15cm} \\text{and } \\, f_{a^{[l-1](i)}_{j},a^{[l-1]}_{k}}(x,y) = f_{a^{[l-1](i)}_{j}}(x)f_{a^{[l-1]}_{k}}(y) \\;\\; \\text{ for } \\, j\\neq{}k \\;\\; \\text{ and } \\, 1 \\leq{} j,k \\leq{} n^{[l-1]} $ \n",
    "##### $ \\hspace{0.15cm} \\text{and } \\, w^{[l]}_{p,q} \\sim{} N(0, \\sigma{}^{2}) \\; \\text{ and } \\; b^{[l]}_{p} = 0 $\n",
    "##### $ \\hspace{0.3cm} z^{[l](i)}_{p} =\\sum^{n^{[l-1]}}_{q=1} w^{[l]}_{p,q}a^{[l-1](i)}_{q} + b^{[l]}_{p} =\\sum^{n^{[l-1]}}_{q=1} w^{[l]}_{p,q}a^{[l-1](i)}_{q} $\n",
    "##### $ \\hspace{0.3cm} \\mathbb{E}[z^{[l](i)}_{p}] = \\;\\;\\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[w^{[l]}_{p,q}a^{[l-1](i)}_{q}] =\\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[w^{[l]}_{p,q}]\\mathbb{E}[a^{[l-1](i)}_{q}] = 0 $\n",
    "##### $ \\hspace{0.3cm} \\text{var}[z^{[l](i)}_{p}] = \\mathbb{E}[(z^{[l](i)}_{p})^{2}] - (\\mathbb{E}[z^{[l](i)}_{p}])^{2} = \\mathbb{E}[(z^{[l](i)}_{p})^{2}] = \\mathbb{E}[(\\sum^{n^{[l-1]}}_{q=1} w^{[l]}_{p,q}a^{[l-1](i)}_{q})^{2}] $ \n",
    "##### $ \\hspace{1.775cm} = \\mathbb{E}[\\sum^{n^{[l-1]}}_{q=1}(w^{[l]}_{p,q}a^{[l-1](i)}_{q})^{2}+2\\sum_{1\\leq{}q<r\\leq{}n^{[l-1]}}w^{[l]}_{p,q}a^{[l-1](i)}_{q}w^{[l]}_{p,r}a^{[l-1](i)}_{r}] \\;\\; $ ($ \\because{} \\, (\\sum^{N}_{i=1}X_{i})^{2} = \\sum^{N}_{i=1}X_{i}^{2} + 2 \\sum_{1\\leq{}q<r\\leq{}N}X_{q}X_{r} $)\n",
    "##### $ \\hspace{1.775cm} =\\sum^{n^{[l-1]}}_{q=1}\\mathbb{E}[(w^{[l]}_{p,q}a^{[l-1](i)}_{q})^{2}]+2\\sum_{1\\leq{}q<r\\leq{}n^{[l-1]}}\\mathbb{E}[w^{[l]}_{p,q}a^{[l-1](i)}_{q}w^{[l]}_{p,r}a^{[l-1](i)}_{r}] $\n",
    "##### $ \\hspace{1.775cm} =\\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[(w^{[l]}_{p,q})^{2}] \\cdot{} \\mathbb{E}[(a^{[l-1](i)}_{q})^{2}] + 2\\sum_{1\\leq{}q<r\\leq{}n^{[l-1]}} \\mathbb{E}[w^{[l]}_{p,q}] \\cdot{} \\mathbb{E}[a^{[l-1](i)}_{q}] \\cdot{} \\mathbb{E}[w^{[l]}_{p,r}] \\cdot{} \\mathbb{E}[a^{[l-1](i)}_{r}] \\;\\; $ ($ \\because{} \\, \\text{independence} $)\n",
    "##### $ \\hspace{1.775cm} =\\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[(w^{[l]}_{p,q})^{2}] \\cdot{} \\mathbb{E}[(a^{[l-1](i)}_{q})^{2}] $\n",
    "##### $ \\hspace{1.775cm} =\\sum^{n^{[l-1]}}_{q=1} \\text{var}[w^{[l]}_{p,q}] \\cdot{} \\text{var}[a^{[l-1](i)}_{q}] $\n",
    "##### $ \\hspace{1.775cm} = n^{[l-1]} \\cdot{} \\text{var}[w^{[l]}_{p,q}] \\cdot{} \\text{var}[a^{[l-1](i)}_{q}] $\n",
    "##### $ \\hspace{1.775cm} = n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} c^{2} $\n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[z^{[l](i)}_{p}] > \\text{var}[a^{[l-1](i)}_{p}] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} c^{2} > c^{2} \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 > 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 순전파 단계에서 레이어를 지날 때 마다 분산이 점점 커져 활성화 함수의 입력 분포가 점점 확대됨\n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[z^{[l](i)}_{p}] < \\text{var}[a^{[l-1](i)}_{p}] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} c^{2} < c^{2} \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 < 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 순전파 단계에서 레이어를 지날 때 마다 분산이 점점 감소해 활성화 함수의 입력 분포가 점점 축소됨\n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 각 레이어 별로 입력, 출력 간 분산이 일정하게 유지되지 않는 경우 학습 과정에서 그래디언트의 방향과 크기가 불안정해져 학습이 어려워짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 역전파에서 입력($ \\nabla{}_{a^{[l-1](i)}_{q}}J $)과 출력($ \\nabla{}_{z^{[l](i)}_{q}}J $)의 분산을 고려하지 않았을 때의 문제점 : \n",
    "##### $ \\hspace{0.15cm} \\text{if } \\; \\mathbb{E}[\\nabla{}_{z^{[l](i)}_{q}}J] = 0, \\;\\; \\text{var}[\\nabla{}_{z^{[l](i)}_{q}}J] < \\infty{} $\n",
    "##### $ \\hspace{0.15cm} \\text{and } \\, f_{\\nabla{}_{z^{[l](i)}_{j}}J,\\nabla{}_{z^{[l](i)}_{k}}J}(x,y) = f_{\\nabla{}_{z^{[l](i)}_{j}}J}(x)f_{\\nabla{}_{z^{[l](i)}_{k}}J}(y) \\;\\; \\text{ for } \\, j\\neq{}k \\;\\; \\text{ and } \\, 1 \\leq{} j,k \\leq{} n^{[l]} $ \n",
    "##### $ \\hspace{0.15cm} \\text{and } \\, w^{[l]}_{p,q} \\sim{} N(0, \\sigma{}^{2}) \\; \\text{ and } \\; b^{[l]}_{p} = 0 $\n",
    "##### $ \\hspace{0.3cm} \\nabla{}_{a^{[l-1](i)}_{q}}J =\\sum^{n^{[l]}}_{p=1} w^{[l]}_{p,q} \\nabla{}_{z^{[l](i)}_{p}}J $\n",
    "##### $ \\hspace{0.3cm} \\mathbb{E}[\\nabla{}_{a^{[l-1](i)}_{q}}J] = \\mathbb{E}[\\sum^{n^{[l]}}_{p=1} w^{[l]}_{p,q} \\nabla{}_{z^{[l](i)}_{p}}J] =\\sum^{n^{[l]}}_{p=1} \\mathbb{E}[w^{[l]}_{p,q}] \\cdot{} \\mathbb{E}[\\nabla{}_{z^{[l](i)}_{p}}J] = 0 $\n",
    "##### $ \\hspace{0.3cm} \\text{var}[\\nabla{}_{a^{[l-1](i)}_{q}}J] = \\mathbb{E}[(\\nabla{}_{a^{[l-1](i)}_{q}}J)^{2}] - (\\mathbb{E}[\\nabla{}_{a^{[l-1](i)}_{q}}J])^{2} = \\mathbb{E}[(\\nabla{}_{a^{[l-1](i)}_{q}}J)^{2}] = \\mathbb{E}[(\\sum^{n^{[l]}}_{p=1} w^{[l]}_{p,q} \\nabla{}_{z^{[l](i)}_{p}}J)^{2}] $\n",
    "##### $ \\hspace{2.375cm} = \\mathbb{E}[\\sum^{n^{[l]}}_{p=1} (w^{[l]}_{p,q} \\nabla{}_{z^{[l](i)}_{p}}J)^{2} + 2\\sum_{1\\leq{}p<r\\leq{}n^{[l]}} w^{[l]}_{p,q} \\nabla{}_{z^{[l](i)}_{p}}J w^{[l]}_{r,q} \\nabla{}_{z^{[l](i)}_{r}}J] $\n",
    "##### $ \\hspace{2.375cm} =\\sum^{n^{[l]}}_{p=1} \\mathbb{E}[(w^{[l]}_{p,q} \\nabla{}_{z^{[l](i)}_{p}}J)^{2}] + 2\\sum_{1\\leq{}p<r\\leq{}n^{[l]}} \\mathbb{E}[w^{[l]}_{p,q} \\nabla{}_{z^{[l](i)}_{p}}J w^{[l]}_{r,q} \\nabla{}_{z^{[l](i)}_{r}}J] $\n",
    "##### $ \\hspace{2.375cm} =\\sum^{n^{[l]}}_{p=1} \\mathbb{E}[(w^{[l]}_{p,q})^{2}] \\cdot{} \\mathbb{E}[(\\nabla{}_{z^{[l](i)}_{p}}J)^{2}] + 2\\sum_{1\\leq{}p<r\\leq{}n^{[l]}} \\mathbb{E}[w^{[l]}_{p,q}] \\cdot{} \\mathbb{E}[\\nabla{}_{z^{[l](i)}_{p}}J] \\cdot{} \\mathbb{E}[w^{[l]}_{r,q}] \\cdot{} \\mathbb{E}[\\nabla{}_{z^{[l](i)}_{r}}J] $\n",
    "##### $ \\hspace{2.375cm} =\\sum^{n^{[l]}}_{p=1} \\mathbb{E}[(w^{[l]}_{p,q})^{2}] \\cdot{} \\mathbb{E}[(\\nabla{}_{z^{[l](i)}_{p}}J)^{2}] $\n",
    "##### $ \\hspace{2.375cm} = n^{[l]} \\cdot{} \\mathbb{E}[(w^{[l]}_{p,q})^{2}] \\cdot{} \\mathbb{E}[(\\nabla{}_{z^{[l](i)}_{p}}J)^{2}] $\n",
    "##### $ \\hspace{2.375cm} = n^{[l]} \\cdot{} \\text{var}[w^{[l]}_{p,q}] \\cdot{} \\text{var}[\\nabla{}_{z^{[l](i)}_{p}}J] $\n",
    "##### $ \\hspace{2.375cm} = n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} \\text{var}[\\nabla{}_{z^{[l](i)}_{p}}J] $ \n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[\\nabla{}_{a^{[l-1](i)}_{q}}J] > \\text{var}[\\nabla{}_{z^{[l](i)}_{q}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} \\text{var}[\\nabla{}_{z^{[l](i)}_{q}}J] \\cdot{} > \\text{var}[\\nabla{}_{z^{[l](i)}_{q}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 > 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 역전파 단계에서 레이어를 지날 때 마다 분산이 점점 커져 그래디언트의 입력 분포가 점점 확대됨\n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[\\nabla{}_{a^{[l-1](i)}_{q}}J] < \\text{var}[\\nabla{}_{z^{[l](i)}_{q}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} \\text{var}[\\nabla{}_{z^{[l](i)}_{q}}J] \\cdot{} < \\text{var}[\\nabla{}_{z^{[l](i)}_{q}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 < 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 역전파 단계에서 레이어를 지날 때 마다 분산이 점점 감소해 그래디언트의 입력 분포가 점점 축소됨\n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 각 레이어 별로 입력, 출력 간 분산이 일정하게 유지되지 않는 경우 학습 과정에서 그래디언트의 방향과 크기가 불안정해져 학습이 어려워짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **자비에 초기화(Xavier Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 네트워크 층의 입력과 출력 수 고려하여 초기화의 범위를 조정하는 초기화 방법\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}+n^{[l]}}) \\,\\, \\text{ or } \\,\\, W^{[l]} \\sim{} U(-\\sqrt{\\frac{6}{n^{[l-1]}+n^{[l]}}},\\sqrt{\\frac{6}{n^{[l-1]}+n^{[l]}}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)**\n",
    "##### $ \\hspace{0.15cm} $ 순전파에서 입력($a^{[l-1]}_{p}$)과 출력($z^{[l]}_{p}$)의 분산을 유지하기 위해서\n",
    "##### $ \\hspace{0.45cm} \\text{var}[a^{[l-1]}_{p}] = \\text{var}[z^{[l](i)}_{p}] $ \n",
    "##### $ \\hspace{0.45cm} \\text{var}[a^{[l-1](i)}_{p}] = n^{[l-1]} \\cdot{} \\text{var}[w^{[l]}_{p,q}] \\cdot{} \\text{var}[a^{[l-1](i)}_{p}] $\n",
    "##### $ \\hspace{0.45cm} 1 = n^{[l-1]} \\cdot{} \\text{var}[w^{[l]}_{p,q}] $ \n",
    "##### $ \\hspace{0.45cm} \\rightarrow{} \\text{var}[w^{[l]}_{p,q}] = \\frac{1}{n^{[l-1]}} $ \n",
    "##### $ \\hspace{0.15cm} $ 역전파에서 입력($ \\nabla{}_{a^{[l-1](i)}_{p}}J $)과 출력($ \\nabla{}_{z^{[l](i)}_{p}}J $)의 분산을 유지하기 위해서\n",
    "##### $ \\hspace{0.45cm} \\text{var}[\\nabla{}_{z^{[l](i)}_{p}}J] = \\text{var}[\\nabla{}_{a^{[l-1]}_{p}}J] $\n",
    "##### $ \\hspace{0.45cm} \\text{var}[\\nabla{}_{z^{[l](i)}_{p}}J] = n^{[l]} \\cdot{} \\text{var}[w^{[l]}_{p,q}] \\cdot{} \\text{var}[\\nabla{}_{z^{[l](i)}_{p}}J] $\n",
    "##### $ \\hspace{0.45cm} 1 = n^{[l]} \\cdot{} \\text{var}[w^{[l]}_{p,q}] $\n",
    "##### $ \\hspace{0.45cm} \\rightarrow{} \\text{var}[w^{[l]}_{p,q}] = \\frac{1}{n^{[l]}} $ \n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 이 두 조건을 만족하도록 타협하여 $ \\text{var}[w^{[l]}_{p,q}] = \\frac{2}{n^{[l-1]}+n^{[l]}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 입력 노드 수($ n^{[l-1]} $)및 출력 노드 수($ n^{[l]} $)를 모두 고려하여 출력(활성화)의 분산이 일정하도록 유도\n",
    "##### $ \\hspace{0.15cm} $ ② 활성화 함수의 출력 대칭성을 가정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 깊은 신경망에서도 (상대적으로) 안정적이며 빠르게 수렴 가능\n",
    "##### $ \\hspace{0.15cm} $ ② 활성화 값과 기울기의 분산을 균형 있게 유지하여 기울기 소실/폭발 **완화**하여 학습 안정성 증가\n",
    "##### $ \\hspace{0.15cm} $ ③ 시그모이드, 하이퍼볼릭 탄젠트 함수를 사용하는 레이어에서 효과적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 렐루 활성화 함수에서 성능 저하 가능성 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **카이밍 초기화(Kaiming Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 렐루(ReLU) 종류의 활성화 함수를 사용하는 신경망에서 층 간 신호 분산을 일정하게 유지하도록 고안\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}}) \\,\\, \\text{ or } \\,\\, W^{[l]} \\sim{} U(-\\sqrt{\\frac{6}{n^{[l-1]}}},\\sqrt{\\frac{6}{n^{[l-1]}}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** 렐루 종류의 활성화함수는 입력의 음수 부분을 $ 0 $으로 만들기 때문에 이를 보정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 렐루 활성화 함수에서 가중치 값의 분산을 효과적으로 유지\n",
    "##### $ \\hspace{0.15cm} $ ② 렐루 계열 활성화 함수의 특성을 반영하여 음의 값이 제거되는 현상을 보정\n",
    "##### $ \\hspace{0.15cm} $ ③ 신경망의 각 층에서 활성화 분산이 크게 줄어들지 않도록 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 기울기 소실 문제를 크게 완화\n",
    "##### $ \\hspace{0.15cm} $ ② 렐루 계열 활성화 함수를 사용하는 레이어에서 효과적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 시그모이드, 하이퍼볼릭 탄젠트 활성화 함수에서는 분산이 커 다소 성능 저하 가능성 존재"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
