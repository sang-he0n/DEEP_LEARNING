{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.1. **Weight Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **가중치 초기화(Weight Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델 학습의 시작 단계에서 네트워크의 각 연결 가중치를 설정하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 레이어 별 노드가 모두 같은 가중치로 시작한다면 모든 노드가 동일하게 출력하기 때문에 노드를 확장하는 의미가 없어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① 무작위 초기화(Random Initialization)\n",
    "##### $ \\hspace{0.15cm} $ ② 자비에 초기화(Xavier Initialization)\n",
    "##### $ \\hspace{0.15cm} $ ③ 카이밍 초기화(Kaiming Initialization)\n",
    "##### $ \\hspace{0.325cm} \\vdots{} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **무작위 초기화(Random Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 학습 전 가중치를 단순히 무작위로 설정하는 초기화 방법\n",
    "#### $ \\Rightarrow{} w \\sim{} N(0, \\, \\sigma{}^{2}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 가중치를 정규분포 혹은 균등분포를 따른다고 가정해 무작위로 설정하여 파라미터 초기값의 대칭성을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 낮은 계산 비용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 초기 가중치가 너무 크거나 작을 경우, 기울기가 소실되거나 폭발할 수 있어 학습 불안정성 초래\n",
    "##### $ \\hspace{0.15cm} $ ② 가중치 스케일에 대한 고려 미비\n",
    "##### $ \\hspace{0.15cm} $ ③ 각 층에 대한 입력과 출력의 분산을 고려하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **(Theorem 2-1) Variance Preservation Property**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1)** 순전파에서 입력 분포와 출력 분포의 분산이 동일하지 않을 경우 학습이 비효율적으로 진행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** 순전파에서 $ l $번 째 레이어의 입력($ a_{q}^{[l-1]} $)과 출력($ z_{p}^{[l]} $)의 분산을 고려하지 않았을 때의 문제점 : \n",
    "##### $ \\hspace{0.15cm} \\text{if } \\; z_{p}^{[l]} = \\textbf{w}^{[l]}_{p} \\textbf{a}^{[l-1]} = \\sum^{n^{[l-1]}}_{q=1} w^{[l]}_{p,q}a^{[l-1]}_{q}, \\;\\; \\textbf{z}^{[l]} = \\begin{bmatrix} z_{1} \\\\ z_{2} \\\\ \\vdots{} \\\\ z_{n^{[l]}} \\end{bmatrix}, \\;\\; \\textbf{w}_{p}^{[l]} = \\begin{bmatrix} w_{p,1} \\\\ w_{p,2} \\\\ \\vdots{} \\\\ w_{p,n^{[l-1]}} \\end{bmatrix}^{T}, \\;\\; \\textbf{a}^{[l-1]} = \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ \\vdots{} \\\\ a_{n^{[l-1]}} \\end{bmatrix} $\n",
    "##### $ \\hspace{0.15cm} \\text{and } \\; w^{[l]}_{p,q} \\sim{} N(0, \\sigma{}^{2}), \\;\\; \\mathbb{E}[a_{q}^{[l-1]}] = 0, \\;\\; \\text{var}[a_{q}^{[l-1]}] = c^{2} < \\infty{} $\n",
    "##### $ \\hspace{0.15cm} \\text{and } \\; w^{[l]}_{p,q} \\perp{} a^{[l-1]}_{q}, \\;\\; w^{[l]}_{p,q} \\perp{} w^{[l]}_{r}, \\;\\; a^{[l]}_{q} \\perp{} a^{[l]}_{r} \\; \\text{ for } \\; q \\neq{} r \\;\\; $ ($ \\text{independent} $)\n",
    "##### $ \\hspace{0.3cm} \\mathbb{E}[z_{p}^{[l]}] = \\mathbb{E}[\\sum^{n^{[l-1]}}_{q=1} w^{[l]}_{p,q}a^{[l-1]}_{q}] = \\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[w^{[l]}_{p,q}a^{[l-1]}_{q}] = \\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[w^{[l]}_{p,q}]\\mathbb{E}[a^{[l-1]}_{q}] = 0 $ \n",
    "##### $ \\hspace{0.3cm} \\text{var}[z_{p}^{[l]}] = \\mathbb{E}[(z_{p}^{[l]})^{2}] - (\\mathbb{E}[z_{p}^{[l]}])^{2} = \\mathbb{E}[(z_{p}^{[l]})^{2}] = \\mathbb{E}[(\\sum^{n^{[l-1]}}_{q=1} w^{[l]}_{p,q}a^{[l-1]}_{q})^{2}] $\n",
    "##### $ \\hspace{1.5cm} = \\mathbb{E}[\\sum^{n^{[l-1]}}_{q=1}(w^{[l]}_{p,q}a^{[l-1]}_{q})^{2} + 2 \\sum^{n^{[l-1]}}_{1\\leq{}q<r\\leq{}n}(w^{[l]}_{p,q}a^{[l-1]}_{q}w^{[l]}_{r}a^{[l-1]}_{r})] \\;\\; \\because{} (\\sum{}_{i=1}^{n} x_{i})^{2} = \\sum{}^{n}_{i=1} x_{i}^{2} + 2\\sum{}_{1\\leq{}i<j\\leq{}n}^{n} x_{i} x_{j} $ \n",
    "##### $ \\hspace{1.5cm} = \\mathbb{E}[\\sum^{n^{[l-1]}}_{q=1}(w^{[l]}_{p,q}a^{[l-1]}_{q})^{2}] + \\mathbb{E}[2\\sum^{n^{[l-1]}}_{1\\leq{}q<r\\leq{}n}(w^{[l]}_{p,q}a^{[l-1]}_{q}w^{[l]}_{r}a^{[l-1]}_{r})] $ \n",
    "##### $ \\hspace{1.5cm} = \\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[(w^{[l]}_{p,q}a^{[l-1]}_{q})^{2}] + 2\\sum^{n^{[l-1]}}_{1\\leq{}q<r\\leq{}n} \\mathbb{E}[w^{[l]}_{p,q}a^{[l-1]}_{q}w^{[l]}_{r}a^{[l-1]}_{r}] $ \n",
    "##### $ \\hspace{1.5cm} = \\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[(w^{[l]}_{p,q})^{2}]\\mathbb{E}[(a^{[l-1]}_{q})^{2}] + \\sum^{n^{[l-1]}}_{\\substack{q,r=1\\\\q\\neq{}r}} \\mathbb{E}[w^{[l]}_{p,q}] \\mathbb{E}[a^{[l-1]}_{q}] \\mathbb{E}[w^{[l]}_{r}] \\mathbb{E}[a^{[l-1]}_{r}] \\;\\; (\\because{} \\text{independent}) $ \n",
    "##### $ \\hspace{1.5cm} = \\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[(w^{[l]}_{p,q})^{2}]\\mathbb{E}[(a^{[l-1]}_{q})^{2}] $\n",
    "##### $ \\hspace{1.5cm} = n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} c^{2} $\n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[z_{p}^{[l]}] > \\text{var}[a^{[l-1]}] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} c^{2} > c^{2} \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 > 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 순전파 단계에서 레이어를 지날 때 마다 분산이 점점 커져 활성화 함수의 입력 분포가 점점 확대됨\n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[z_{p}^{[l]}] < \\text{var}[a^{[l-1]}] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} c^{2} < c^{2} \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 < 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 순전파 단계에서 레이어를 지날 때 마다 분산이 점점 감소해 활성화 함수의 입력 분포가 점점 축소됨\n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 각 레이어 별로 입-출력 간 분산이 일정하지 않는 경우 활성화 함수의 미분이 급격히 작아지거나 커질 수 있어 그래디언트 소실, 폭발이 발생할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2)** 순전파 역시 입력 분포와 출력 분포의 분산이 동일하지 않을 경우 학습이 비효율적으로 진행된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** 역전파에서 입력($ \\nabla{}_{a_{q}^{[l-1]}}J $)과 출력($ \\nabla{}_{z_{p}^{[l]}}J $)의 분산을 고려하지 않았을 때의 문제점 : \n",
    "##### $ \\hspace{0.15cm} \\text{if } \\; w^{[l]}_{p,q} \\perp{} \\nabla{}_{z_{p}^{[l]}}J, \\;\\; w^{[l]}_{p,q} \\perp{} w^{[l]}_{p,r}, \\;\\; \\nabla{}_{z_{p}^{[l]}}J \\perp{} \\nabla{}_{z_{r}^{[l]}}J \\; \\text{ for } \\; p \\neq{} r \\;\\; $ ($ \\text{independent} $)\n",
    "##### $ \\hspace{0.3cm} \\nabla_{a_{q}^{[l-1]}} J = \\sum{}^{n^{[l]}}_{p=1} \\frac{\\partial{}J}{\\partial{}z_{p}^{[l]}} \\cdot{} \\frac{\\partial{}z_{p}^{[l]}}{\\partial{}a^{l-1}_{q}} = \\sum{}^{n^{[l]}}_{p=1} \\nabla{}_{z_{p}^{[l]}}J \\cdot{} w_{p,q} $\n",
    "##### $ \\hspace{0.3cm} \\mathbb{E}[\\nabla{}_{a^{[l-1]}_{q}}J] = \\mathbb{E}[\\sum{}^{n^{[l]}}_{p=1} \\nabla{}_{z_{p}^{[l]}}J \\cdot{} w_{p,q}] = \\sum{}^{n^{[l]}}_{p=1} \\mathbb{E}[\\nabla{}_{z_{p}^{[l]}}J \\cdot{} w_{p,q}] = \\sum{}^{n^{[l]}}_{p=1} \\mathbb{E}[\\nabla{}_{z_{p}^{[l]}}J] \\mathbb{E}[w_{p,q}] = 0 $\n",
    "##### $ \\hspace{0.3cm} \\text{var}[\\nabla{}_{a^{[l-1]}_{q}}J] = \\mathbb{E}[(\\nabla{}_{a^{[l-1]}_{q}}J)^{2}] - (\\mathbb{E}[\\nabla{}_{a^{[l-1]}_{q}}J])^{2} = \\mathbb{E}[(\\nabla{}_{a^{[l-1]}_{q}}J)^{2}] = \\mathbb{E}[(\\sum{}^{n^{[l]}}_{p=1} \\nabla{}_{z_{p}^{[l]}}J \\cdot{} w_{p,q})^{2}] $\n",
    "##### $ \\hspace{2.15cm} = \\mathbb{E}[\\sum{}^{n^{[l]}}_{p=1} (\\nabla{}_{z_{p}^{[l]}}J \\cdot{} w_{p,q})^{2} + 2 \\sum{}^{n^{[l]}}_{1\\leq{}p<r\\leq{}n} \\nabla{}_{z_{p}^{[l]}}J \\cdot{} w_{p,q} \\cdot{} \\nabla{}_{z_{r}^{[l]}}J \\cdot{} w_{r,q}] $\n",
    "##### $ \\hspace{2.15cm} = \\mathbb{E}[\\sum{}^{n^{[l]}}_{p=1} (\\nabla{}_{z_{p}^{[l]}}J \\cdot{} w_{p,q})^{2}] + \\mathbb{E}[2 \\sum{}^{n^{[l]}}_{1\\leq{}p<r\\leq{}n} \\nabla{}_{z_{p}^{[l]}}J \\cdot{} w_{p,q} \\cdot{} \\nabla{}_{z_{r}^{[l]}}J \\cdot{} w_{r,q}] $\n",
    "##### $ \\hspace{2.15cm} = \\sum{}^{n^{[l]}}_{p=1} \\mathbb{E}[(\\nabla{}_{z_{p}^{[l]}}J \\cdot{} w_{p,q})^{2}] + 2 \\sum{}^{n^{[l]}}_{1\\leq{}p<r\\leq{}n} \\mathbb{E}[\\nabla{}_{z_{p}^{[l]}}J \\cdot{} w_{p,q} \\cdot{} \\nabla{}_{z_{r}^{[l]}}J \\cdot{} w_{r,q}] $\n",
    "##### $ \\hspace{2.15cm} = \\sum{}^{n^{[l]}}_{p=1} \\mathbb{E}[(\\nabla{}_{z_{p}^{[l]}}J)^{2}] \\mathbb{E}[(w_{p,q})^{2}] + 2 \\sum{}^{n^{[l]}}_{1\\leq{}p<r\\leq{}n} \\mathbb{E}[\\nabla{}_{z_{p}^{[l]}}J] \\mathbb{E}[ w_{p,q}] \\mathbb{E}[\\nabla{}_{z_{r}^{[l]}}J] \\mathbb{E}[w_{r,q}] $\n",
    "##### $ \\hspace{2.15cm} = \\sum{}^{n^{[l]}}_{p=1} \\mathbb{E}[(\\nabla{}_{z_{p}^{[l]}}J)^{2}] \\mathbb{E}[(w_{p,q})^{2}] $ \n",
    "##### $ \\hspace{2.15cm} = n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} c^{2} $ \n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[\\nabla{}_{a^{[l-1]}_{q}}J] > \\text{var}[\\nabla{}_{z^{[l]}_{p}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} \\text{var}[\\nabla{}_{z^{[l]}_{p}}J] \\cdot{} > \\text{var}[\\nabla{}_{z^{[l]}_{p}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 > 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 역전파 단계에서 레이어를 지날 때 마다 분산이 점점 커져 그래디언트의 입력 분포가 점점 확대됨\n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[\\nabla{}_{a^{[l-1]}_{q}}J] < \\text{var}[\\nabla{}_{z^{[l]}_{p}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} \\text{var}[\\nabla{}_{z^{[l]}_{p}}J] \\cdot{} < \\text{var}[\\nabla{}_{z^{[l]}_{p}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 < 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 역전파 단계에서 레이어를 지날 때 마다 분산이 점점 감소해 그래디언트의 입력 분포가 점점 축소됨\n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 각 레이어 별로 입력, 출력 간 분산이 일정하게 유지되지 않는 경우 학습 과정에서 그래디언트의 방향과 크기가 불안정해져 학습이 어려워짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **자비에 초기화(Xavier Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 네트워크 층의 입력 분포와 출력 분포를 분산을 고려하여 초기화의 범위를 조정하는 초기화 방법\n",
    "#### $ \\Rightarrow{} w \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}+n^{[l]}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 입력 노드 수($ n^{[l-1]} $)및 출력 노드 수($ n^{[l]} $)를 모두 고려하여 출력(활성화)의 분산이 일정하도록 유도\n",
    "##### $ \\hspace{0.15cm} $ ② 선형 또는 시그모이드, 하이퍼볼릭 탄젠트와 같이 출력 평균이 $ 0 $에 가까운 활성화 함수에서 유효"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 깊은 신경망에서도 (상대적으로) 안정적이며 빠르게 수렴 가능\n",
    "##### $ \\hspace{0.15cm} $ ② 활성화 값과 기울기의 분산을 균형 있게 유지하여 기울기 소실/폭발 안화해 학습 안정성 증가\n",
    "##### $ \\hspace{0.15cm} $ ③ 시그모이드, 하이퍼볼릭 탄젠트 함수를 사용하는 레이어에서 효과적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 렐루 활성화 함수에서 성능 저하 가능성 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **카이밍 초기화(Kaiming Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 렐루(ReLU) 종류의 활성화 함수를 사용하는 신경망에서 층 간 신호 분산을 일정하게 유지하도록 고안\n",
    "#### $ \\Rightarrow{} w \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** 렐루 종류의 활성화함수는 입력의 음수 부분을 $ 0 $으로 만들어 분산이 기존 대비 절반이 되고 되고 이를 보정함\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\text{var}[\\text{ReLU}(z)] = \\frac{1}{2} \\text{var}[z] \\;\\; \\text{ s.t.} \\; z \\sim{} N(0, \\sigma{}^{2}) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 렐루 계열 활성화 함수의 특성을 반영하여 음의 값이 제거되는 현상에 대해 보정 처리\n",
    "##### $ \\hspace{0.15cm} $ ② 신경망의 각 층에서 출력 분산이 크게 줄어들지 않도록 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 입력 및 출력 분산을 유지하여 역전파 안정성 보장\n",
    "##### $ \\hspace{0.15cm} $ ② 렐루 계열 활성화 함수를 사용하는 레이어에서 효과적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 시그모이드, 하이퍼볼릭 탄젠트 활성화 함수에서는 분산이 커져 다소 성능 저하 가능성 존재"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
