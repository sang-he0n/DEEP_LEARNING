{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.1. **Weight Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **가중치 초기화(Weight Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델 학습의 시작 단계에서 네트워크의 각 연결 가중치를 설정하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 레이어 별 노드가 모두 같은 가중치로 시작한다면 모든 노드가 동일하게 출력하기 때문에 노드를 확장하는 의미가 없어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① 무작위 초기화(Random Initialization)\n",
    "##### $ \\hspace{0.15cm} $ ② 자비에 초기화(Xavier Initialization)\n",
    "##### $ \\hspace{0.15cm} $ ③ 히 초기화(He Initialization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **무작위 초기화(Random Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 학습 전 가중치를 단순히 무작위로 설정하는 초기화 방법\n",
    "#### $ \\Rightarrow{} w \\sim{} N(0, \\, \\sigma{}^{2}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 가중치를 정규분포 혹은 균등분포를 따른다고 가정해 무작위로 설정하여 파라미터 초기값의 대칭성을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 낮은 계산 비용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 초기 가중치가 너무 크거나 작을 경우, 기울기가 소실되거나 폭발할 수 있어 학습 불안정성 초래\n",
    "##### $ \\hspace{0.15cm} $ ② 가중치 스케일에 대한 고려 미비\n",
    "##### $ \\hspace{0.15cm} $ ③ 각 층에 대한 입력과 출력의 분산을 고려하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 순전파에서 $ l $번 째 레이어의 입력($ a^{[l-1]} $)과 출력($ z^{[l]} $)의 분산을 고려하지 않았을 때의 문제점 : \n",
    "##### $ \\hspace{0.15cm} \\text{if } \\; w^{[l]}_{q} \\sim{} N(0, \\sigma{}^{2}), \\;\\; \\mathbb{E}[a^{[l-1]}] = 0, \\;\\; \\text{var}[a^{[l-1]}] = c^{2} < \\infty{} $\n",
    "##### $ \\hspace{0.15cm} \\text{and } \\; z^{[l]} = \\sum^{n^{[l-1]}}_{q=1} w^{[l]}_{q}a^{[l-1]}_{q} $\n",
    "##### $ \\hspace{0.15cm} \\text{and } \\; w^{[l]}_{q} \\perp{} a^{[l-1]}_{p}, \\;\\; w^{[l]}_{q} \\perp{} w^{[l-1]}_{p}, \\;\\; a^{[l]}_{q} \\perp{} a^{[l-1]}_{p} \\;\\; $ ($ \\text{independent} $)\n",
    "##### $ \\hspace{0.3cm} \\mathbb{E}[z^{[l]}] = \\mathbb{E}[\\sum^{n^{[l-1]}}_{q=1} w^{[l]}_{q}a^{[l-1]}_{q}] = \\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[w^{[l]}_{q}a^{[l-1]}_{q}] = \\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[w^{[l]}_{q}]\\mathbb{E}[a^{[l-1]}_{q}] = 0 $ \n",
    "##### $ \\hspace{0.3cm} \\text{var}[z^{[l]}] = \\mathbb{E}[(z^{[l]})^{2}] - (\\mathbb{E}[z^{[l]}])^{2} = \\mathbb{E}[(z^{[l]})^{2}] = \\mathbb{E}[(\\sum^{n^{[l-1]}}_{q=1} w^{[l]}_{q}a^{[l-1]}_{q})^{2}] $\n",
    "##### $ \\hspace{1.5cm} = \\mathbb{E}[\\sum^{n^{[l-1]}}_{q=1}(w^{[l]}_{q}a^{[l-1]}_{q})^{2} + \\sum^{n^{[l-1]}}_{\\substack{q,r=1\\\\q\\neq{}r}}(w^{[l]}_{q}a^{[l-1]}_{q}w^{[l]}_{r}a^{[l-1]}_{r})] \\;\\; \\because{} (\\sum{}_{i=1}^{n} x_{i})^{2} = \\sum{}^{n}_{i=1} x_{i}^{2} + \\sum{}_{\\substack{i,j=1 \\\\ i \\neq{} j}}^{n} x_{i} x_{j} $ \n",
    "##### $ \\hspace{1.5cm} = \\mathbb{E}[\\sum^{n^{[l-1]}}_{q=1}(w^{[l]}_{q}a^{[l-1]}_{q})^{2}] + \\mathbb{E}[\\sum^{n^{[l-1]}}_{\\substack{q,r=1\\\\q\\neq{}r}}(w^{[l]}_{q}a^{[l-1]}_{q}w^{[l]}_{r}a^{[l-1]}_{r})] $ \n",
    "##### $ \\hspace{1.5cm} = \\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[(w^{[l]}_{q}a^{[l-1]}_{q})^{2}] + \\sum^{n^{[l-1]}}_{\\substack{q,r=1\\\\q\\neq{}r}} \\mathbb{E}[w^{[l]}_{q}a^{[l-1]}_{q}w^{[l]}_{r}a^{[l-1]}_{r}] $ \n",
    "##### $ \\hspace{1.5cm} = \\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[(w^{[l]}_{q})^{2}]\\mathbb{E}[(a^{[l-1]}_{q})^{2}] + \\sum^{n^{[l-1]}}_{\\substack{q,r=1\\\\q\\neq{}r}} \\mathbb{E}[w^{[l]}_{q}] \\mathbb{E}[a^{[l-1]}_{q}] \\mathbb{E}[w^{[l]}_{r}] \\mathbb{E}[a^{[l-1]}_{r}] \\;\\; (\\because{} \\text{independent}) $ \n",
    "##### $ \\hspace{1.5cm} = \\sum^{n^{[l-1]}}_{q=1} \\mathbb{E}[(w^{[l]}_{q})^{2}]\\mathbb{E}[(a^{[l-1]}_{q})^{2}] $\n",
    "##### $ \\hspace{1.5cm} = n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} c^{2} $\n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[z^{[l]}] > \\text{var}[a^{[l-1]}] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} c^{2} > c^{2} \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 > 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 순전파 단계에서 레이어를 지날 때 마다 분산이 점점 커져 활성화 함수의 입력 분포가 점점 확대됨\n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[z^{[l]}] < \\text{var}[a^{[l-1]}] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} c^{2} < c^{2} \\;\\; \\Leftrightarrow{} \\;\\; n^{[l-1]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 < 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 순전파 단계에서 레이어를 지날 때 마다 분산이 점점 감소해 활성화 함수의 입력 분포가 점점 축소됨\n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 각 레이어 별로 입-출력 간 분산이 일정하지 않는 경우 활성화 함수의 미분이 급격히 작아지거나 커질 수 있어 그래디언트 소실, 폭발이 발생할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 역전파에서 입력($ \\nabla{}_{a^{[l-1]}_{q}}J $)과 출력($ \\nabla{}_{z^{[l]}_{q}}J $)의 분산을 고려하지 않았을 때의 문제점 : \n",
    "##### $ \\hspace{0.15cm} \\text{if } \\; \\mathbb{E}[\\nabla{}_{z^{[l]}_{q}}J] = 0, \\;\\; \\text{var}[\\nabla{}_{z^{[l]}_{q}}J] < \\infty{} $\n",
    "##### $ \\hspace{0.15cm} \\text{and } \\; f_{\\nabla{}_{z^{[l]}_{j}}J,\\nabla{}_{z^{[l]}_{k}}J}(x,y) = f_{\\nabla{}_{z^{[l]}_{j}}J}(x)f_{\\nabla{}_{z^{[l]}_{k}}J}(y) \\;\\; \\text{ for } \\, j\\neq{}k \\;\\; \\text{ and } \\; 1 \\leq{} j,k \\leq{} n^{[l]} $ \n",
    "##### $ \\hspace{0.15cm} \\text{and } \\; w^{[l]}_{p,q} \\sim{} N(0, \\sigma{}^{2}) \\; \\text{ and } \\; b^{[l]}_{p} = 0 $\n",
    "##### $ \\hspace{0.3cm} \\nabla{}_{a^{[l-1]}_{q}}J =\\sum^{n^{[l]}}_{p=1} w^{[l]}_{p,q} \\nabla{}_{z^{[l]}_{p}}J $\n",
    "##### $ \\hspace{0.3cm} \\mathbb{E}[\\nabla{}_{a^{[l-1]}_{q}}J] = \\mathbb{E}[\\sum^{n^{[l]}}_{p=1} w^{[l]}_{p,q} \\nabla{}_{z^{[l]}_{p}}J] =\\sum^{n^{[l]}}_{p=1} \\mathbb{E}[w^{[l]}_{p,q}] \\cdot{} \\mathbb{E}[\\nabla{}_{z^{[l]}_{p}}J] = 0 $\n",
    "##### $ \\hspace{0.3cm} \\text{var}[\\nabla{}_{a^{[l-1]}_{q}}J] = \\mathbb{E}[(\\nabla{}_{a^{[l-1]}_{q}}J)^{2}] - (\\mathbb{E}[\\nabla{}_{a^{[l-1]}_{q}}J])^{2} = \\mathbb{E}[(\\nabla{}_{a^{[l-1]}_{q}}J)^{2}] = \\mathbb{E}[(\\sum^{n^{[l]}}_{p=1} w^{[l]}_{p,q} \\nabla{}_{z^{[l]}_{p}}J)^{2}] $\n",
    "##### $ \\hspace{2.375cm} = \\mathbb{E}[\\sum^{n^{[l]}}_{p=1} (w^{[l]}_{p,q} \\nabla{}_{z^{[l]}_{p}}J)^{2} + 2\\sum_{1\\leq{}p<r\\leq{}n^{[l]}} w^{[l]}_{p,q} \\nabla{}_{z^{[l]}_{p}}J w^{[l]}_{r,q} \\nabla{}_{z^{[l]}_{r}}J] $\n",
    "##### $ \\hspace{2.375cm} =\\sum^{n^{[l]}}_{p=1} \\mathbb{E}[(w^{[l]}_{p,q} \\nabla{}_{z^{[l]}_{p}}J)^{2}] + 2\\sum_{1\\leq{}p<r\\leq{}n^{[l]}} \\mathbb{E}[w^{[l]}_{p,q} \\nabla{}_{z^{[l]}_{p}}J w^{[l]}_{r,q} \\nabla{}_{z^{[l]}_{r}}J] $\n",
    "##### $ \\hspace{2.375cm} =\\sum^{n^{[l]}}_{p=1} \\mathbb{E}[(w^{[l]}_{p,q})^{2}] \\cdot{} \\mathbb{E}[(\\nabla{}_{z^{[l]}_{p}}J)^{2}] + 2\\sum_{1\\leq{}p<r\\leq{}n^{[l]}} \\mathbb{E}[w^{[l]}_{p,q}] \\cdot{} \\mathbb{E}[\\nabla{}_{z^{[l]}_{p}}J] \\cdot{} \\mathbb{E}[w^{[l]}_{r,q}] \\cdot{} \\mathbb{E}[\\nabla{}_{z^{[l]}_{r}}J] $\n",
    "##### $ \\hspace{2.375cm} =\\sum^{n^{[l]}}_{p=1} \\mathbb{E}[(w^{[l]}_{p,q})^{2}] \\cdot{} \\mathbb{E}[(\\nabla{}_{z^{[l]}_{p}}J)^{2}] $\n",
    "##### $ \\hspace{2.375cm} = n^{[l]} \\cdot{} \\mathbb{E}[(w^{[l]}_{p,q})^{2}] \\cdot{} \\mathbb{E}[(\\nabla{}_{z^{[l]}_{p}}J)^{2}] $\n",
    "##### $ \\hspace{2.375cm} = n^{[l]} \\cdot{} \\text{var}[w^{[l]}_{p,q}] \\cdot{} \\text{var}[\\nabla{}_{z^{[l]}_{p}}J] $\n",
    "##### $ \\hspace{2.375cm} = n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} \\text{var}[\\nabla{}_{z^{[l]}_{p}}J] $ \n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[\\nabla{}_{a^{[l-1]}_{q}}J] > \\text{var}[\\nabla{}_{z^{[l]}_{q}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} \\text{var}[\\nabla{}_{z^{[l]}_{q}}J] \\cdot{} > \\text{var}[\\nabla{}_{z^{[l]}_{q}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 > 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 역전파 단계에서 레이어를 지날 때 마다 분산이 점점 커져 그래디언트의 입력 분포가 점점 확대됨\n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[\\nabla{}_{a^{[l-1]}_{q}}J] < \\text{var}[\\nabla{}_{z^{[l]}_{q}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} \\text{var}[\\nabla{}_{z^{[l]}_{q}}J] \\cdot{} < \\text{var}[\\nabla{}_{z^{[l]}_{q}}J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot{} \\sigma{}^{2} \\cdot{} 1 < 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} $ 역전파 단계에서 레이어를 지날 때 마다 분산이 점점 감소해 그래디언트의 입력 분포가 점점 축소됨\n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 각 레이어 별로 입력, 출력 간 분산이 일정하게 유지되지 않는 경우 학습 과정에서 그래디언트의 방향과 크기가 불안정해져 학습이 어려워짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 역전파에서 입력($ \\nabla_{a^{[l-1]}_q} J $)과 출력($ \\nabla_{z^{[l]}_q} J $)의 분산을 고려하지 않았을 때의 문제점 :  \n",
    "##### $ \\hspace{0.15cm} \\text{if } \\; \\mathbb{E}[\\nabla_{z^{[l]}_q} J] = 0, \\;\\; \\text{var}[\\nabla_{z^{[l]}_q} J] < \\infty $\n",
    "##### $ \\hspace{0.15cm} \\text{and } \\; f_{\\nabla_{z^{[l]}_j} J,\\nabla_{z^{[l]}_k} J}(x,y) = f_{\\nabla_{z^{[l]}_j} J}(x) f_{\\nabla_{z^{[l]}_k} J}(y) \\;\\; \\text{for } j \\neq k \\;\\; \\text{and } \\; 1 \\leq j,k \\leq n^{[l]} $\n",
    "##### $ \\hspace{0.15cm} \\text{and } \\; w^{[l]}_{p,q} \\sim \\mathcal{N}(0, \\sigma^2), \\;\\; b^{[l]}_p = 0 $\n",
    "##### $ \\hspace{0.3cm} \\nabla_{a^{[l-1]}_q} J = \\sum_{p=1}^{n^{[l]}} w^{[l]}_{p,q} \\nabla_{z^{[l]}_p} J $\n",
    "##### $ \\hspace{0.3cm} \\mathbb{E}[\\nabla_{a^{[l-1]}_q} J] = \\sum_{p=1}^{n^{[l]}} \\mathbb{E}[w^{[l]}_{p,q}] \\cdot \\mathbb{E}[\\nabla_{z^{[l]}_p} J] = 0 $\n",
    "##### $ \\hspace{0.3cm} \\text{var}[\\nabla_{a^{[l-1]}_q} J] = \\mathbb{E}[(\\nabla_{a^{[l-1]}_q} J)^2] = \\mathbb{E}\\left[\\left(\\sum_{p=1}^{n^{[l]}} w^{[l]}_{p,q} \\nabla_{z^{[l]}_p} J\\right)^2\\right] $\n",
    "##### $ \\hspace{2.375cm} = \\sum_{p=1}^{n^{[l]}} \\mathbb{E}[(w^{[l]}_{p,q} \\nabla_{z^{[l]}_p} J)^2] + 2 \\sum_{1 \\le p < r \\le n^{[l]}} \\mathbb{E}[w^{[l]}_{p,q} \\nabla_{z^{[l]}_p} J \\cdot w^{[l]}_{r,q} \\nabla_{z^{[l]}_r} J] $\n",
    "##### $ \\hspace{2.375cm} = \\sum_{p=1}^{n^{[l]}} \\mathbb{E}[(w^{[l]}_{p,q})^2] \\cdot \\mathbb{E}[(\\nabla_{z^{[l]}_p} J)^2] + 2 \\sum_{1 \\le p < r \\le n^{[l]}} \\mathbb{E}[w^{[l]}_{p,q}] \\cdot \\mathbb{E}[\\nabla_{z^{[l]}_p} J] \\cdot \\mathbb{E}[w^{[l]}_{r,q}] \\cdot \\mathbb{E}[\\nabla_{z^{[l]}_r} J] $\n",
    "##### $ \\hspace{2.375cm} = \\sum_{p=1}^{n^{[l]}} \\sigma^2 \\cdot \\text{var}[\\nabla_{z^{[l]}_p} J] $\n",
    "##### $ \\hspace{2.375cm} = n^{[l]} \\cdot \\sigma^2 \\cdot \\text{var}[\\nabla_{z^{[l]}_p} J] $\n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[\\nabla_{a^{[l-1]}_q} J] > \\text{var}[\\nabla_{z^{[l]}_q} J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot \\sigma^2 > 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\; \\text{역전파 단계에서 레이어를 지날 때마다 분산이 점점 커져 그래디언트가 폭주함} $\n",
    "##### $ \\hspace{0.3cm} \\text{if } \\; \\text{var}[\\nabla_{a^{[l-1]}_q} J] < \\text{var}[\\nabla_{z^{[l]}_q} J] \\;\\; \\Leftrightarrow{} \\;\\; n^{[l]} \\cdot \\sigma^2 < 1 $\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\; \\text{역전파 단계에서 레이어를 지날 때마다 분산이 점점 작아져 그래디언트가 소실됨} $\n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 각 레이어 별로 입력, 출력 간 분산이 일정하게 유지되지 않으면 학습 과정에서 그래디언트의 방향과 크기가 불안정해져 학습이 어려워짐\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **자비에 초기화(Xavier Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 네트워크 층의 입력과 출력 수 고려하여 초기화의 범위를 조정하는 초기화 방법\n",
    "#### $ \\Rightarrow{} w \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}+n^{[l]}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)**\n",
    "##### $ \\hspace{0.15cm} $ 순전파에서 입력($a^{[l-1]}_{p}$)과 출력($z^{[l]}_{p}$)의 분산을 유지하기 위해서\n",
    "##### $ \\hspace{0.45cm} \\text{var}[a^{[l-1]}_{p}] = \\text{var}[z^{[l](i)}_{p}] $ \n",
    "##### $ \\hspace{0.45cm} \\text{var}[a^{[l-1](i)}_{p}] = n^{[l-1]} \\cdot{} \\text{var}[w^{[l]}_{p,q}] \\cdot{} \\text{var}[a^{[l-1](i)}_{p}] $\n",
    "##### $ \\hspace{0.45cm} 1 = n^{[l-1]} \\cdot{} \\text{var}[w^{[l]}_{p,q}] $ \n",
    "##### $ \\hspace{0.45cm} \\rightarrow{} \\text{var}[w^{[l]}_{p,q}] = \\frac{1}{n^{[l-1]}} $ \n",
    "##### $ \\hspace{0.15cm} $ 역전파에서 입력($ \\nabla{}_{a^{[l-1](i)}_{p}}J $)과 출력($ \\nabla{}_{z^{[l](i)}_{p}}J $)의 분산을 유지하기 위해서\n",
    "##### $ \\hspace{0.45cm} \\text{var}[\\nabla{}_{z^{[l](i)}_{p}}J] = \\text{var}[\\nabla{}_{a^{[l-1]}_{p}}J] $\n",
    "##### $ \\hspace{0.45cm} \\text{var}[\\nabla{}_{z^{[l](i)}_{p}}J] = n^{[l]} \\cdot{} \\text{var}[w^{[l]}_{p,q}] \\cdot{} \\text{var}[\\nabla{}_{z^{[l](i)}_{p}}J] $\n",
    "##### $ \\hspace{0.45cm} 1 = n^{[l]} \\cdot{} \\text{var}[w^{[l]}_{p,q}] $\n",
    "##### $ \\hspace{0.45cm} \\rightarrow{} \\text{var}[w^{[l]}_{p,q}] = \\frac{1}{n^{[l]}} $ \n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 이 두 조건을 만족하도록 타협하여 $ \\text{var}[w^{[l]}_{p,q}] = \\frac{2}{n^{[l-1]}+n^{[l]}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 입력 노드 수($ n^{[l-1]} $)및 출력 노드 수($ n^{[l]} $)를 모두 고려하여 출력(활성화)의 분산이 일정하도록 유도\n",
    "##### $ \\hspace{0.15cm} $ ② 활성화 함수의 출력 대칭성을 가정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 깊은 신경망에서도 (상대적으로) 안정적이며 빠르게 수렴 가능\n",
    "##### $ \\hspace{0.15cm} $ ② 활성화 값과 기울기의 분산을 균형 있게 유지하여 기울기 소실/폭발 **완화**하여 학습 안정성 증가\n",
    "##### $ \\hspace{0.15cm} $ ③ 시그모이드, 하이퍼볼릭 탄젠트 함수를 사용하는 레이어에서 효과적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 렐루 활성화 함수에서 성능 저하 가능성 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **카이밍 초기화(Kaiming Initialization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 렐루(ReLU) 종류의 활성화 함수를 사용하는 신경망에서 층 간 신호 분산을 일정하게 유지하도록 고안\n",
    "#### $ \\Rightarrow{} w \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** 렐루 종류의 활성화함수는 입력의 음수 부분을 $ 0 $으로 만들기 때문에 이를 보정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 렐루 활성화 함수에서 가중치 값의 분산을 효과적으로 유지\n",
    "##### $ \\hspace{0.15cm} $ ② 렐루 계열 활성화 함수의 특성을 반영하여 음의 값이 제거되는 현상을 보정\n",
    "##### $ \\hspace{0.15cm} $ ③ 신경망의 각 층에서 활성화 분산이 크게 줄어들지 않도록 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 기울기 소실 문제를 크게 완화\n",
    "##### $ \\hspace{0.15cm} $ ② 렐루 계열 활성화 함수를 사용하는 레이어에서 효과적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 시그모이드, 하이퍼볼릭 탄젠트 활성화 함수에서는 분산이 커 다소 성능 저하 가능성 존재"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
