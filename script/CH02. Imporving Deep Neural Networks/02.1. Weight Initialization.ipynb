{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.1. **Weight Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Weight Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델 학습의 시작 단계에서 네트워크의 각 연결 가중치를 설정하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① Random Initialization\n",
    "##### $ \\hspace{0.15cm} $ ② Xavier(Glorot) Initialization\n",
    "##### $ \\hspace{0.15cm} $ ③ He Initialization\n",
    "##### $ \\hspace{0.15cm} $ ④ LeCun Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Xavier Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : tanh와 같이 출력이 $ \\, 0 $ 을 중심으로 대칭적인 활성화 함수를 사용하는 신경망에서 층간 분산을 일정하게 유지하도록 고안\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}+n^{[l]}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 입력 뉴런 수($ n^{[l-1]} $)및 출력 뉴런 수($ n^{[l]} $)를 모두 고려하여 분산을 조절\n",
    "##### $ \\hspace{0.15cm} $ ② 활성화 함수의 대칭성을 가정함\n",
    "##### $ \\hspace{0.15cm} $ ③ 층을 지나면서 활성화의 분산이 급격히 변하지 않도록 조절"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **He Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : ReLU 류 활성화 함수를 사용하는 신경망에서 층 간 신호 분산을 일정하게 유지하도록 고안\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\frac{2}{n^{[l-1]}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 입력 뉴런 수만 고려하여 분산을 조절\n",
    "##### $ \\hspace{0.15cm} $ ② ReLU 계열 활성화 함수의 특성을 반영하여 음의 값이 제거되는 현상을 보정\n",
    "##### $ \\hspace{0.15cm} $ ③ 신경망의 각 층에서 활성화 분산이 크게 줄어들지 않도록 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **LeCun Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : SELU 활성화 함수와 같이 초기 활성화 분포의 분산을 안정적으로 유지\n",
    "#### $ \\Rightarrow{} W^{[l]} \\sim{} N(0, \\, \\frac{1}{n^{[l-1]}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 입력 뉴런 수에 의해서만 분산을 조정\n",
    "##### $ \\hspace{0.15cm} $ ② 네트워크의 초기 활성화 분포를 안정적으로 유지하는 데 중점\n",
    "##### $ \\hspace{0.15cm} $ ③ 특히 SELU 활성화 함수와 함께 self-normalizing 특성을 극대화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** self-normalizing : 신경망 학습 과정에서 각 층을 거치면서 활성화 출력의 분포(평균 및 분산)가 자연스럽게 일정하게 유지되는 특성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
