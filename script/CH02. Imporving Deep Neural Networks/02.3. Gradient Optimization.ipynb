{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.3. **Gradient Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **그래디언트 최적화(Gradient Optimization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 확률적 경사하강법의 단점(노이즈, 학습률 문제)을 보완하기 위해 고안된 경사하강법 최적화 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 배치 경사하강법의 경우 그래디언트의 노이즈가 상대적으로 작아, 최적화 방법을 사용할 필요성이 적음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** : \n",
    "##### $ \\hspace{0.15cm} $ ① 모멘텀(Momentum)\n",
    "##### $ \\hspace{0.15cm} $ ② 네스테로프 가속 경사(Nesterov Acclearted Gradient)\n",
    "##### $ \\hspace{0.15cm} $ ③ 아다그라드(AdaGrad;Adaptive Gradient;NAG)\n",
    "##### $ \\hspace{0.15cm} $ ④ Root Mean Square Propagation(RMSProp)\n",
    "##### $ \\hspace{0.15cm} $ ⑤ 아담(Adaptive Moment Estimation;Adam)\n",
    "##### $ \\hspace{0.15cm} $ ⑥ 가중치감쇠 아담(Adam with Weight Decay;AdamW)\n",
    "##### $ \\hspace{0.15cm} $ ⑦ 학습률 스케쥴링(Learning Rate Scheduling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **모멘텀(Momentum)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 이전 단계들의 그라디언트를 누적하여 부드러운 방향으로 최적화를 유도하는 방법\n",
    "#### $ \\Rightarrow{} \\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\cdot{} v^{[l]}_{t} \\;\\; \\text{ where } \\; v^{[l]}_{t} = \\beta{}_{1} \\cdot{} v^{[l]}_{t-1} + \\nabla{}_{\\theta{}^{[l]}_{t}}J(\\theta{}^{[l]}_{t}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 지수가중이동평균(exponential weighted moving average)을 이용하면 초기 편향 문제를 일부 완화할 수 있음\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\cdot{} v^{[l]}_{t} \\;\\; \\text{ where } \\; v^{[l]}_{t} = \\beta{}_{1} \\cdot{} v^{[l]}_{t-1} + (1 - \\beta{}_{1} ) \\nabla{}_{\\theta{}^{[l]}_{t}}J(\\theta{}^{[l]}_{t}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 이 때 $ v^{[l]}_{t} $ 를 속도(velocity)라고 정의됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 속도가 그래디언트의 과거 정보를 누적하여 현재 업데이트에 반영함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 방향이 자주 바뀌는 그라디언트에 대해 순간적인 진동(jitter)를 줄여 학습 속도가 빠르고 안정적으로 수렴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① $ \\beta{}_{1} $ 은 하이퍼 파라미터이기에 경험적으로 설정해야함 (일반적으로 $ 0.9 \\sim{} 0.99 $)\n",
    "##### $ \\hspace{0.15cm} $ ② 잘못된 방향으로 크게 가속될 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **네스테로프 가속 경사(Nesterov Acclearted Gradient;NAG)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 미리 다음 위치를 예측하여 그 지점에서의 그래디언트를 계산하여 최적화하는 방법\n",
    "#### $ \\Rightarrow{} \\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\cdot{} v^{[l]}_{t} \\;\\; \\text{ where } \\; v^{[l]}_{t} = \\beta{}_{1} \\cdot{} v^{[l]}_{t-1} + \\nabla{}_{\\tilde{\\theta{}}^{[l]}_{t}}J(\\tilde{\\theta{}^{[l]}_{t}}) $\n",
    "#### $ \\hspace{3.55cm} \\text{and } \\; \\tilde{\\theta{}^{[l]}_{t}} = \\theta{}^{[l]}_{t}+\\beta{}_{1}v^{[l]}_{t-1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 사전 예측 개념을 도입하여 더 정확한 방향성을 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 모멘텀과 NAG의 시각적 비교\n",
    "<div style=\"text-align:center;\">\n",
    "  <figure style=\"display:inline-block; \">\n",
    "    <img src=\"../../img/02.3. Gradient Opt. Algorithms (1).png\" width=\"120%\">\n",
    "    <figcaption>모멘텀</figcaption>\n",
    "  </figure>\n",
    "  <figure style=\"display:inline-block;\">\n",
    "    <img src=\"../../img/02.3. Gradient Opt. Algorithms (2).png\" width=\"120%\">\n",
    "    <figcaption>네스테로프 가속 경사</figcaption>\n",
    "  </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① (모멘텀 대비) 더 빠르게 수렴 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① $ \\beta{}_{1} $ 은 하이퍼 파라미터이기에 경험적으로 설정해야함 (일반적으로 $ 0.9 \\sim{} 0.99 $)\n",
    "##### $ \\hspace{0.15cm} $ ② (모멘텀 대비) 계산량 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **아다그라드(AdaGrad;Adaptive Gradient)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 개별 파라미터마다 학습률을 그래디언트의 제곱을 이용해 적응적으로 설정하는 방법\n",
    "#### $ \\Rightarrow{} \\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\cdot{} \\frac{1}{\\sqrt{s^{[l]}_{t}+\\epsilon{}}} \\cdot{} \\nabla{}_{\\theta{}^{[l]}_{t}}J(\\theta{}^{[l]}_{t}) \\;\\; \\text{ where } \\; s^{[l]}_{t} =  s^{[l]}_{t-1} + (\\nabla{}_{\\theta{}^{[l]}_{t}}J(\\theta{}^{[l]}_{t}))^{2} $\n",
    "##### $ \\hspace{6.4cm} \\text{and } \\; \\epsilon{} \\approx{} 10^{-7} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 개별 파라미터의 학습률에 대해 그라디언트의 제곱을 이용해 적응적으로 설정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① (모멘텀 대비) 개별 파라미터의 그래디언트가 큰 방향은 학습률이 크게 감소하고, 그 반대는 크게 증가하기에 더 빠르게 수렴 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 학습이 진행됨에 따라 누적 제곱 그래디언트($ s^{[l]}_{t} $)이 커지게 되고 곧 학습률이 너무 작아져 업데이트가 잘 되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Root Mean Square Propagation(RMSProp)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 아다그라드에서 그래디언트의 제곱을 지수가중이동평균을 이용하여 학습률을 설정하는 방법\n",
    "#### $ \\Rightarrow{} \\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\cdot{} \\frac{1}{\\sqrt{s^{[l]}_{t}+\\epsilon{}}} \\cdot{} \\nabla{}_{\\theta{}^{[l]}_{t}}J(\\theta{}^{[l]}_{t}) \\;\\; \\text{ where } \\; s^{[l]}_{t} = \\beta{}_{2} \\cdot{} s^{[l]}_{t-1} + (1 - \\beta{}_{2} ) \\cdot{} (\\nabla{}_{\\theta{}^{[l]}_{t}}J(\\theta{}^{[l]}_{t}))^{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 편향 보정(bias correction) : 지수가중이동평균 값 **[CONTENT]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 최근 그래디언트 위주로 이용하여 학습률을 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 장기적으로도 적당한 학습률 유지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① $ \\beta{}_{2} $ 은 하이퍼 파라미터이기에 경험적으로 설정해야함 (일반적으로 $ 0.9 $)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **아담(Adaptive Moment Estimation;Adam)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모멘텀과 RMSProp을 모두 이용하여 학습 방향의 안정성과 학습률을 조정하는 방법\n",
    "#### $ \\Rightarrow{} \\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\cdot{} \\frac{1}{\\sqrt{s^{[l]}_{t}+\\epsilon{}}} \\cdot{} v^{[l]}_{t} \\;\\; \\text{ where } \\; v^{[l]}_{t} = \\beta{}_{1} \\cdot{} v^{[l]}_{t-1} + (1 - \\beta{}_{1} ) \\cdot{} \\nabla{}_{\\theta{}^{[l]}_{t}}J(\\theta{}^{[l]}_{t}) $ \n",
    "#### $ \\hspace{4.8cm} \\text{and } \\; s^{[l]}_{t} = \\beta{}_{2} \\cdot{} s^{[l]}_{t-1} + (1 - \\beta{}_{2} ) \\cdot{} (\\nabla{}_{\\theta{}^{[l]}_{t}}J(\\theta{}^{[l]}_{t}))^{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 모멘텀 성분으로 인해 이전 기울기의 정보를 누적, 급격한 진동 없이 일관된 방향으로 빠르게 수렴\n",
    "##### $ \\hspace{0.15cm} $ ② RMSProp 성분으로 인해 각 파라미터의 기울기 크기에 따라 학습률을 자동으로 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 안정성 우수해 빠른 수렴 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 여전히 지역 최소(local minima) 혹은 안장점(saddle point)에 빠질 가능성 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **가중치감쇠 아담(Adam with Weight Decay;AdamW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 아담에서 L2 가중치 감쇠(weight decay)를 **분리적으로** 적용하여 정규화를 수행하는 방법\n",
    "#### $ \\Rightarrow{} \\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\cdot{} (\\frac{1}{\\sqrt{s^{[l]}_{t}+\\epsilon{}}} \\cdot{} v^{[l]}_{t} + \\lambda{}\\theta{}^{[l]}_{t}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 손실 함수에 정규화 항을 추가하지 않고, 파라미터 업데이트 단계에서 정규화 항 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 정규화 항이 업데이트 단계에서 직접 적용되어 adaptive scaling($ \\frac{1}{\\sqrt{s^{[l]}_{t}+\\epsilon{}}} $)의 영향을 받지 않아 정규화 효과가 균일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **학습률 스케줄링(Learning Rate Scheduling)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 훈련 과정에서 학습률을 적응적(동적)으로 변경하여 성능을 개선하는 전략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** : \n",
    "##### $ \\hspace{0.15cm} $ ① 학습률 감쇠 : 초반에 큰 보폭으로 빠르게 학습하다가 후반에는 작은 보폭으로 학습률을 점진적으로 줄여나가는 기법\n",
    "##### $ \\hspace{0.3cm} \\cdot{} $ 단계적 감쇠(Step decay) : 일정 에포크(epoch)마다 고정된 비율로 학습률 감소\n",
    "##### $ \\hspace{0.3cm} \\cdot{} $ 지수 감쇠(Exponential decay) : 지수적으로 매 학습 스텝($ t $)마다 학습률을 감소\n",
    "##### $ \\hspace{0.3cm} \\cdot{} $ 코사인 어닐링(Cosine Annealing) : 주기적 진동을 코사인 곡선 형태로 주어 학습률을 부드럽게 조정\n",
    "##### $ \\hspace{0.15cm} $ ② 학습률 웜업(warm-up) : 초반에 학습률을 낮게 시작하여 점진적으로 증가시켜 손실이 발산하는 것을 막는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 특징** : 학습이 진행됨에 따라 적응적으로 학습률을 설정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 초기 빠른 수렴 속도으로 인한 성능 향상 \n",
    "##### $ \\hspace{0.15cm} $ ② (일반적으로) 학습이 진행될수록 학습률이 작아지기에 수렴 가능성 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 추가적인 스케쥴링 방식 및 적절한 하이퍼파라미터 설정 필요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
