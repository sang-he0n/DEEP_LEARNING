{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.5. **Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **정규화(Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망의 각 계층에 입력되는 데이터의 분포를 표준화하여 학습을 안정화하고 가속화하는 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① 입력 정규화(Input Normalization)\n",
    "##### $ \\hspace{0.15cm} $ ② 배치 정규화(Batch Normalization)\n",
    "##### $ \\hspace{0.15cm} $ ③ 레이어 정규화(Layer Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **입력 정규화(Input Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력 데이터를 샘플별로 정규화하여 평균을 $ \\, 0 $ 으로 표준편차를 $ \\, 1 $ 로 조정하는 방법\n",
    "#### $ \\Rightarrow{} \\textbf{x}^{(i)} = \\frac{\\textbf{x}^{(i)} - \\bar{\\textbf{x}}}{\\sigma{}_{\\textbf{x}}} \\;\\; \\text{ where } \\, \\bar{\\textbf{x}} = \\displaystyle\\frac{1}{m} \\displaystyle\\sum^{m}_{i=1}\\textbf{x}^{(i)} \\;\\; \\text{and } \\, \\sigma{}_{\\textbf{x}} = \\sqrt{\\frac{1}{m} \\displaystyle\\sum^{m}_{i=1}(\\textbf{x}^{(i)}-\\bar{\\textbf{x}})^{2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 입력 데이터의 스케일을 통일하여 학습 효율 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 안정적인 그래디언트 제공으로 학습 속도 향상\n",
    "##### $ \\hspace{0.15cm} $ ② 모델의 성능 및 일반화 성능 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 정적 방식이기에 입력 데이터 분포 변화에 대응하기 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **배치 정규화(Batch Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망에서 각 레이어 별로 선형 변환 출력($ \\textbf{z}^{[l](i)} $)에 대해 샘플 별로 정규화를 수행하는 방법\n",
    "#### $ \\Rightarrow{} \\tilde{\\textbf{z}}^{[l](i)} = \\gamma{}^{[l]} \\textbf{z}^{[l](i)}_{\\text{norm}} + \\beta{}^{[l]} \\;\\; \\text{ where } \\, \\textbf{z}^{[l](i)}_{\\text{norm}} = \\frac{\\textbf{z}^{[l](i)}-\\bar{\\textbf{z}}^{[l]}}{\\sqrt{(\\sigma{}_{\\textbf{z}^{[l]}})^{2}+\\epsilon{}}} $\n",
    "#### $ \\hspace{4cm} \\text{and } \\, \\bar{\\textbf{z}}^{[l]} = \\displaystyle\\frac{1}{m} \\displaystyle\\sum^{m}_{i=1}\\textbf{z}^{[l](i)} \\;\\; \\text{and } \\, \\sigma{}_{\\textbf{z}^{[l]}} = \\sqrt{\\frac{1}{m} \\displaystyle\\sum^{m}_{i=1}(\\textbf{z}^{[l](i)}-\\bar{\\textbf{z}}^{[l]})^{2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 미니 배치 단위로 레이어 별 선형 결합 출력에 동적 정규화 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 가중치 초기화의 의존성 감소\n",
    "##### $ \\hspace{0.15cm} $ ② 더 큰 학습률 허용으로 빠른 수렴 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 배치 크기가 작을 경우 성능이 저하됨\n",
    "##### $ \\hspace{0.15cm} $ ② 시계열 데이터에서는 효과가 제한적일 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 내부 공변량 이동(Internal Covariate Shift)** : 신경망 내부에서 각 층에 입력되는 활성화 값의 분포가 학습 과정에서 지속적으로 변화하는 현상 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 최근 연구에서는 배치 정규화가 내부 공변량 이동을 감소하지 못하고, 내부 공변량 이동이 발생하더라도 학습에 영향을 미치지 않는다고 주장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **레이어 정규화(Layer Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망에서 각 레이어 별로 선형 변환 출력($ \\textbf{z}^{[l](i)} $)에 대해 **한 샘플의** 특징(노드) 별로 정규화를 수행하는 방법\n",
    "#### $ \\Rightarrow{} \\tilde{\\textbf{z}}^{[l](i)} = \\gamma{}^{[l]} \\textbf{z}^{[l](i)}_{\\text{norm}} + \\beta{}^{[l]} \\;\\; \\text{ where } \\, \\textbf{z}^{[l](i)}_{\\text{norm}} = \\frac{\\textbf{z}^{[l](i)}-\\bar{\\textbf{z}}^{[l](i)}}{\\sqrt{(\\sigma{}_{\\textbf{z}^{[l](i)}})^{2}+\\epsilon{}}} $\n",
    "#### $ \\hspace{4cm} \\text{and } \\, \\bar{\\textbf{z}}^{[l](i)} = \\displaystyle\\frac{1}{n^{[l]}} \\displaystyle\\sum^{n^{[l]}}_{p=1}z^{[l](i)}_{p} \\cdot{} \\underset{1\\times{}n^{[l]}}{\\textbf{1}} \\;\\; \\text{and } \\, \\sigma{}_{\\textbf{z}^{[l](i)}} = \\sqrt{\\frac{1}{n^{[l]}} \\displaystyle\\sum^{n^{[l]}}_{p=1}(z^{[l](i)}_{p}-\\bar{z}^{[l]}_{p})^{2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 작은 배치 크기에서도 안정적\n",
    "##### $ \\hspace{0.15cm} $ ② 배치 통계량이 필요 없으므로 학습과 추론 시 성능 일관성 유지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 일부 CNN 모델 등 공간적 정보(Spatial information)가 중요한 경우 성능 저하될 수 있음.\n",
    "##### $ \\hspace{0.15cm} $ ② 정규화가 지나치게 강하게 적용될 경우 표현력 제한 가능성 존재"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
