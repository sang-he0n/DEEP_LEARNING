{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.5. **Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **정규화(Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망의 각 계층에 입력되는 데이터의 분포를 표준화하여 학습을 안정화하고 가속화하는 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① 입력 정규화(Input Normalization)\n",
    "##### $ \\hspace{0.15cm} $ ② 배치 정규화(Batch Normalization)\n",
    "##### $ \\hspace{0.15cm} $ ③ 레이어 정규화(Layer Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **입력 정규화(Input Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력 데이터를 샘플별로 정규화하여 평균을 $ \\, 0 $ 으로 표준편차를 $ \\, 1 $ 로 조정하는 방법\n",
    "#### $ \\Rightarrow{} \\textbf{x}^{(i)} = \\frac{\\textbf{x}^{(i)} - \\bar{\\textbf{x}}}{\\sigma{}_{\\textbf{x}}} \\;\\; \\text{ where } \\, \\bar{\\textbf{x}} =\\frac{1}{m}\\sum^{m}_{i=1}\\textbf{x}^{(i)} \\;\\; \\text{and } \\, \\sigma{}_{\\textbf{x}} = \\sqrt{\\frac{1}{m}\\sum^{m}_{i=1}(\\textbf{x}^{(i)}-\\bar{\\textbf{x}})^{2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 입력 데이터의 스케일을 통일하여 학습 효율 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 안정적인 그래디언트 제공으로 학습 속도 향상\n",
    "##### $ \\hspace{0.15cm} $ ② 모델의 성능 및 일반화 성능 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 정적 방식이기에 입력 데이터 분포 변화에 대응하기 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **배치 정규화(Batch Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망에서 각 레이어 별로 선형 변환 출력($ \\textbf{z}^{[l](i)} $)에 대해 미니 배치 내에서 각 노드(특징) 별로 정규화 및 아핀 변환하는 방법\n",
    "#### $ \\Rightarrow{} \\tilde{\\textbf{z}}^{[l](i)} = \\gamma{}^{[l]} \\textbf{z}^{[l](i)}_{\\text{norm}} + \\beta{}^{[l]} \\;\\; \\text{ where } \\, \\textbf{z}^{[l](i)}_{\\text{norm}} = \\frac{\\textbf{z}^{[l](i)}-\\bar{\\textbf{z}}^{[l]}}{\\sqrt{(\\sigma{}_{\\textbf{z}^{[l]}})^{2}+\\epsilon{}}} $\n",
    "#### $ \\hspace{4cm} \\text{and } \\, \\bar{\\textbf{z}}^{[l]} =\\frac{1}{B}\\sum^{B}_{i=1}\\textbf{z}^{[l](i)} \\;\\; \\text{and } \\, \\sigma{}_{\\textbf{z}^{[l]}} = \\sqrt{\\frac{1}{B}\\sum^{B}_{i=1}(\\textbf{z}^{[l](i)}-\\bar{\\textbf{z}}^{[l]})^{2}} $\n",
    "#### $ \\hspace{4cm} \\text{and } B \\, \\text{ is mini-batch size}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 아핀 변환(affine transformation)** : 선형 변환과 평행 이동을 결합한 함수\n",
    "##### $ \\Rightarrow{} f(\\textbf{x}) = A\\textbf{x} + \\textbf{b} \\;\\; \\text{ where } \\, \\textbf{x} \\in{} \\mathbb{R}^{n} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 미니 배치 단위로 레이어 별 선형 결합 출력에 동적 정규화($ \\, \\gamma{}^{[l]}, \\, \\beta{}^{[l]} $ 가 학습가능한 파라미터) 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 가중치 초기화의 의존성 감소\n",
    "##### $ \\hspace{0.15cm} $ ② 더 큰 학습률 허용으로 빠른 수렴 가능\n",
    "##### $ \\hspace{0.15cm} $ ③ 내부 공변량 이동 해소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 내부 공변량 이동(Internal Covariate Shift)** : 학습 과정에서 각 층의 입력(이전 층의 활성화 값) 분포가 계속 변화해 후속 층에도 영향을 주는 현상 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 최근 연구에서는 배치 정규화가 내부 공변량 이동을 감소하지 못하고, 손실 함수 지형(loss landscape)의 부드러움(smoothness) 개선과 연관됨\n",
    "##### $ \\hspace{0.15cm} $ By ([Santurkar et al., \"How Does Batch Normalization Help Optimization?\", NeurIPS 2018.](https://arxiv.org/abs/1805.11604))\n",
    "##### $ \\hspace{0.15cm} $ ① 기존 믿음(가설) : 배치 정규화는 각 층(layer)의 입력 분포를 안정화시켜 내부 공변량 이동을 줄여 학습 성능 향상\n",
    "##### $ \\hspace{0.15cm} $ ② 실험적 결과\n",
    "##### $ \\hspace{0.45cm} \\cdot{} $ 논문의 실험 결과 배치 정규화를 써도 실제 층 입력 분포가 그렇게 안정되지 않는 경우 존재 \n",
    "##### $ \\hspace{0.45cm} \\cdot{} $ 일부러 잡음(noise)을 넣어서 내부 공변량 변화를 훨씬 심하게 만들었는데도 성능이 떨어지지 않았음\n",
    "##### $ \\hspace{0.45cm} \\rightarrow{} $ 배치 정규화의 진짜 효과는 내부 공변량 이동 감소와는 직접적인 연관성이 없음\n",
    "##### $ \\hspace{0.15cm} $ ③ 실제 배치 정규화 효과 : **손실 지형(loss landscape)을 더욱 매끄럽게 만드는 효과**\n",
    "##### $ \\hspace{0.45cm} \\cdot{} $ 손실함수의 립시츠 연속성(Lipschitz continuity) 개선\n",
    "##### $ \\hspace{0.45cm} \\rightarrow{} ||\\nabla{}_{\\tilde{\\textbf{z}}^{[l]}}J ||_{2}^{2} \\leq{} \\frac{(\\gamma{}^{[l]})^{2}}{(\\sigma{}_{\\textbf{z}^{[l]}})^{2}}(||\\nabla{}_{\\textbf{z}^{[l]}}J||_{2}^{2} - \\underbrace{\\frac{1}{B}< \\textbf{1}, \\nabla{}_{\\textbf{z}^{[l]}}J>^{2}}_{ⓐ} - \\underbrace{\\frac{1}{B} <\\nabla{}_{\\textbf{z}^{[l]}}J, \\textbf{z}^{[l]}_{\\text{norm}}>^{2}}_{ⓑ} ) $\n",
    "##### $ \\hspace{0.9cm} $ ⓐ : 원본 그래디언트($ \\nabla{}_{\\textbf{z}^{[l]}}J $)의 평균 성분(모든 요소의 평균)의 크기\n",
    "##### $ \\hspace{0.9cm} $ ⓑ : 원본 그래디언트와 정규화된 활성값($\\textbf{z}^{[l]}_{\\text{norm}}$) 사이의 내적(유사도)\n",
    "##### $ \\hspace{0.45cm} \\rightarrow{} $ 원본 그래디언트 크기 및 정규화 활성값과의 관계를 통해 그래디언트의 크기를 제한하여 연속성 개선 \n",
    "##### $ \\hspace{0.45cm} \\cdot{} $ 손실함수의 기울기($ \\beta{} $-smoothness) 개선\n",
    "##### $ \\hspace{0.45cm} \\rightarrow{} (\\nabla{}_{\\tilde{\\textbf{z}}^{[l]}}J)^{T} H_{\\tilde{\\textbf{z}}^{[l]}\\tilde{\\textbf{z}}^{[l]}}  (\\nabla{}_{\\tilde{\\textbf{z}}^{[l]}}J) \\leq{} \\frac{(\\gamma{}^{[l]})^{2}}{(\\sigma{}_{\\textbf{z}^{[l]}})^{2}} [\\underbrace{(\\nabla{}_{\\textbf{z}^{[l]}}J)^{T}H_{\\textbf{z}^{[l]}\\textbf{z}^{[l]}}(\\nabla{}_{\\textbf{z}^{[l]}}J)}_{ⓐ} - \\underbrace{\\frac{\\gamma{}^{[l]}}{B} <\\nabla{}_{\\textbf{z}^{[l]}}J, \\textbf{z}^{[l]}_{\\text{norm}}>^{2} ||\\nabla{}_{\\tilde{\\textbf{z}}^{[l]}}J ||_{2}^{2}}_{ⓑ} ] $\n",
    "##### $ \\hspace{0.9cm} \\text{where } \\, H_{\\tilde{\\textbf{z}}^{[l]}\\tilde{\\textbf{z}}^{[l]}} = \\frac{\\partial{}^{2}J}{\\partial{}\\tilde{\\textbf{z}}^{[l]}\\partial{}\\tilde{\\textbf{z}}^{[l]}}  $\n",
    "##### $ \\hspace{0.9cm} $ ⓐ : 원본 그래디언트 헤시안의 2차형(hessian quadratic form)\n",
    "##### $ \\hspace{0.9cm} $ ⓑ : 원본 그래디언트와 정규화된 활성값의 내적과 배치 정규화된 그래언트 크기의 곱\n",
    "##### $ \\hspace{0.45cm} \\rightarrow{} $ 손실의 급격한 변화를 일으키는 불안정한 경사면을 방지하여, 그래디언트가 더 예측 가능하고 안정적인 방향을 가지도록 유도\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} $ 손실 지형의 부드러움을 높여 최적화 과정을 안정화하고, 더 큰 보폭(learning rate)을 사용하여 빠르게 손실을 감소시킬 수 있게 돕는 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 립시츠 연속성의 의미** : 파라미터의 변화에 따른 함수 $ J(\\Theta{}) $ 의 변화가 $ \\, K $ 이하로 제한됨\n",
    "##### $ \\hspace{0.15cm} = $ 함수 $ J(\\Theta{}) $ 가 갑자기 크게 변화하지 않음(부드럽게 변화함)\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\frac{|J(\\Theta{}_{1})-J(\\Theta{}_{2})|}{||\\Theta{}_{1}-\\Theta{}_{2}||_{F}} \\leq{} K \\;\\; \\text{ where } \\Theta{}_{k} = \\{ W^{[1]}_{k}, B^{[1]}_{k}, W^{[2]}_{k}, B^{[1]}_{k}, \\cdots{}, W^{[L]}_{k}, B^{[L]}_{k} \\} \\, \\text{ and } \\, K \\geq{} 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 베타-매끄러움($ \\beta{} $-smoothness)** : 파라미터의 변화에 따른 함수 $ J(\\Theta{}) $ 의 그래디언트 변화가 $ \\, \\beta{} $ 이하로 제한됨\n",
    "##### $ \\hspace{0.15cm} = $ 함수 $ J(\\Theta{}) $ 의 그래디언트가 갑자기 크게 변화하지 않음(부드럽게 변화함)\n",
    "##### $ \\hspace{0.15cm} \\rightarrow{} \\frac{|\\nabla{}_{\\Theta{}_{1}}J-\\nabla{}_{\\Theta{}_{2}}J|}{||\\Theta{}_{1}-\\Theta{}_{2}||_{F}} \\leq{} \\beta{} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 배치 크기가 작을 경우 성능이 저하됨\n",
    "##### $ \\hspace{0.15cm} $ ② 시계열 데이터에서는 효과가 제한적일 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 추론 단계에서는 $ \\, \\gamma{}^{[l]}, \\, \\beta{}^{[l]} $ 에 대하여 지수가중이동평균을 이용하여 처리함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **레이어 정규화(Layer Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망에서 각 레이어 별로 선형 변환 출력($ \\textbf{z}^{[l](i)} $)에 대해 **하나의 샘플 내에서** 모든 노드(특징)에 대해 정규화 및 아핀 변환하는 방법\n",
    "#### $ \\Rightarrow{} \\tilde{\\textbf{z}}^{[l](i)} = \\gamma{}^{[l]} \\textbf{z}^{[l](i)}_{\\text{norm}} + \\beta{}^{[l]} \\;\\; \\text{ where } \\, \\textbf{z}^{[l](i)}_{\\text{norm}} = \\frac{\\textbf{z}^{[l](i)}-\\bar{\\textbf{z}}^{[l](i)}}{\\sqrt{(\\sigma{}_{\\textbf{z}^{[l](i)}})^{2}+\\epsilon{}}} $\n",
    "#### $ \\hspace{4cm} \\text{and } \\, \\bar{\\textbf{z}}^{[l](i)} =\\frac{1}{n^{[l]}}\\sum^{n^{[l]}}_{p=1}z^{[l](i)}_{p} \\cdot{} \\underset{1\\times{}n^{[l]}}{\\textbf{1}} \\;\\; \\text{and } \\, \\sigma{}_{\\textbf{z}^{[l](i)}} = \\sqrt{\\frac{1}{n^{[l]}}\\sum^{n^{[l]}}_{p=1}(z^{[l](i)}_{p}-\\bar{z}^{[l]}_{p})^{2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 동일 샘플 인덱스 내 모든 노드(feature)에 대해 정규화하기 때문에 배치 크기와 무관하게 일정한 성능을 유지함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 작은 배치 크기에서도 안정적\n",
    "##### $ \\hspace{0.15cm} $ ② 배치 통계량이 필요 없으므로 학습과 추론 시 성능 일관성 유지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 일부 컨볼루션 네트워크(CNN) 모델 등 공간적 정보(Spatial information)가 중요한 경우 성능 저하될 수 있음.\n",
    "##### $ \\hspace{0.15cm} $ ② 정규화가 지나치게 강하게 적용될 경우 표현력 제한 가능성 존재"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
