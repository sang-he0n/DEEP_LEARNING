{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.5. **Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **정규화(Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망의 각 계층에 입력되는 데이터의 분포를 표준화하여 학습을 안정화하고 가속화하는 기술"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① 입력 정규화(Input Normalization)\n",
    "##### $ \\hspace{0.15cm} $ ② 배치 정규화(Batch Normalization)\n",
    "##### $ \\hspace{0.15cm} $ ③ 레이어 정규화(Layer Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **입력 정규화(Input Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력 데이터를 샘플별로 정규화하여 평균을 $ 0 $으로 표준편차를 $ 1 $로 조정하는 방법\n",
    "#### $ \\Rightarrow{} \\tilde{\\textbf{x}}^{(i)} = \\frac{\\textbf{x}^{(i)} - \\bar{\\textbf{x}}}{\\sigma{}_{\\textbf{x}}} \\;\\; \\text{ where } \\; \\bar{\\textbf{x}} =\\frac{1}{m}\\sum^{m}_{i=1}\\textbf{x}^{(i)} \\;\\; \\text{and } \\; \\sigma{}_{\\textbf{x}} = \\sqrt{\\frac{1}{m}\\sum^{m}_{i=1}(\\textbf{x}^{(i)}-\\bar{\\textbf{x}})^{2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 입력 데이터의 스케일을 통일하여 학습 효율 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 안정적인 그래디언트 제공으로 학습 속도 향상\n",
    "##### $ \\hspace{0.15cm} $ ② 모델의 성능 및 일반화 성능 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 정적 방식이기에 입력 데이터 분포 변화에 대응하기 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **배치 정규화(Batch Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망에서 각 레이어 별로 선형 변환 출력($ \\textbf{z}^{[l](i)} $)에 대해 미니 배치 내에서 각 노드(특징) 별로 정규화 및 아핀 변환하는 방법\n",
    "#### $ \\Rightarrow{} \\tilde{\\textbf{z}}^{[l](i)} = \\gamma{}^{[l]} \\textbf{z}^{[l](i)}_{\\text{norm}} + \\beta{}^{[l]} \\;\\; \\text{ where } \\; \\textbf{z}^{[l](i)}_{\\text{norm}} = \\frac{\\textbf{z}^{[l](i)}-\\bar{\\textbf{z}}^{[l]}}{\\sqrt{(\\sigma{}_{\\textbf{z}^{[l]}})^{2}+\\epsilon{}}} $\n",
    "#### $ \\hspace{4cm} \\text{and } \\; \\bar{\\textbf{z}}^{[l]} =\\frac{1}{B}\\sum^{B}_{i=1}\\textbf{z}^{[l](i)} \\;\\; \\text{and } \\; \\sigma{}_{\\textbf{z}^{[l]}} = \\sqrt{\\frac{1}{B}\\sum^{B}_{i=1}(\\textbf{z}^{[l](i)}-\\bar{\\textbf{z}}^{[l]})^{2}} $\n",
    "#### $ \\hspace{4cm} \\text{and } B \\, \\text{ is mini-batch size}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 아핀 변환(Affine Transformation)** : 선형 변환과 평행 이동을 결합한 함수\n",
    "##### $ \\Rightarrow{} f(\\textbf{x}) = A\\textbf{x} + \\textbf{b} \\;\\; \\text{ where } \\; \\textbf{x} \\in{} \\mathbb{R}^{n} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 미니 배치 단위로 레이어 별 선형 결합 출력에 동적 정규화($ \\gamma{}^{[l]}, \\, \\beta{}^{[l]} $ 가 학습가능한 파라미터) 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 가중치 초기화의 의존성 감소\n",
    "##### $ \\hspace{0.15cm} $ ② 더 큰 학습률 허용으로 빠른 수렴 가능\n",
    "##### $ \\hspace{0.15cm} $ ③ 내부 공변량 이동 해소 (원 논문 주장)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 내부 공변량 이동(Internal Covariate Shift)** : 학습 과정에서 각 층의 입력(이전 층의 활성화 값) 분포가 계속 변화해 후속 층에도 영향을 주는 현상 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 최근 연구에서는 배치 정규화가 내부 공변량 이동을 감소하지 못하고, 손실 함수 지형(loss landscape)의 부드러움(smoothness) 개선과 연관됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 배치 크기가 작을 경우 성능이 저하됨\n",
    "##### $ \\hspace{0.15cm} $ ② 시계열 데이터에서는 효과가 제한적일 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 추론 단계에서는 $ \\gamma{}^{[l]}, \\, \\beta{}^{[l]} $ 에 대하여 지수가중이동평균을 이용하여 처리함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **레이어 정규화(Layer Normalization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망에서 각 레이어 별로 선형 변환 출력($ \\textbf{z}^{[l](i)} $)에 대해 **하나의 샘플 내에서** 모든 노드(특징)에 대해 정규화 및 아핀 변환하는 방법\n",
    "#### $ \\Rightarrow{} \\tilde{\\textbf{z}}^{[l](i)} = \\gamma{}^{[l]} \\textbf{z}^{[l](i)}_{\\text{norm}} + \\beta{}^{[l]} \\;\\; \\text{ where } \\; \\textbf{z}^{[l](i)}_{\\text{norm}} = \\frac{\\textbf{z}^{[l](i)}-\\bar{\\textbf{z}}^{[l](i)}}{\\sqrt{(\\sigma{}_{\\textbf{z}^{[l](i)}})^{2}+\\epsilon{}}} $\n",
    "#### $ \\hspace{4cm} \\text{and } \\; \\bar{\\textbf{z}}^{[l](i)} =\\frac{1}{n^{[l]}}\\sum^{n^{[l]}}_{p=1}z^{[l](i)}_{p} \\cdot{} \\underset{1\\times{}n^{[l]}}{\\textbf{1}} \\;\\; \\text{and } \\; \\sigma{}_{\\textbf{z}^{[l](i)}} = \\sqrt{\\frac{1}{n^{[l]}}\\sum^{n^{[l]}}_{p=1}(z^{[l](i)}_{p}-\\bar{z}^{[l]}_{p})^{2}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 동일 샘플 인덱스 내 모든 노드(feature)에 대해 정규화하기 때문에 배치 크기와 무관하게 일정한 성능을 유지함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 작은 배치 크기에서도 안정적\n",
    "##### $ \\hspace{0.15cm} $ ② 배치 통계량이 필요 없으므로 학습과 추론 시 성능 일관성 유지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 일부 컨볼루션 네트워크(CNN) 모델 등 공간적(spatial) 정보가 중요한 경우 성능 저하될 수 있음.\n",
    "##### $ \\hspace{0.15cm} $ ② 정규화가 지나치게 강하게 적용될 경우 표현력 제한 가능성 존재"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
