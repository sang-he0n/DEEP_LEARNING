{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.4. **Regularizations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **규제(Regularization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 과적합(overfitting) 방지와 일반화 성능 향상을 위해 사용되는 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① 가중치 감쇠(Weight decay)\n",
    "##### $ \\hspace{0.45cm} \\cdot{} $ 라쏘(Lasso;L1 Regularization)\n",
    "##### $ \\hspace{0.45cm} \\cdot{} $ 릿지(Ridge;L2 Regularization)\n",
    "##### $ \\hspace{0.45cm} \\cdot{} $ 엘라스틱넷(ElasticNet;L1+L2 Regularization)\n",
    "##### $ \\hspace{0.15cm} $ ② 드롭 아웃(Dropout)\n",
    "##### $ \\hspace{0.15cm} $ ③ 조기 종료(Early Stopping)\n",
    "##### $ \\hspace{0.15cm} $ ④ 데이터 증강(Data Argumentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **라쏘(Lasso;L1 Regularization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 비용 함수에 가중치 절댓값 놈(L1 norm)을 추가하는 방법\n",
    "##### $ \\hspace{0.15cm} $ ① Forward propagation(Cost Computation)\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} J(W^{[1]},\\, \\textbf{b}^{[1]},\\, \\cdots{},\\, W^{[L]},\\, \\textbf{b}^{[L]}) = \\frac{1}{m}\\sum^{m}_{i=1} \\ell{}^{(i)} + \\frac{\\lambda{}}{m}\\sum^{L}_{l=1} || W^{[l]} ||_{1} $\n",
    "##### $ \\hspace{5.25cm} = \\frac{1}{m}\\sum^{m}_{i=1} \\ell{}^{(i)} + \\frac{\\lambda{}}{m}\\sum^{L}_{l=1}\\sum^{i=n^{[l]}}_{i=1}\\sum^{n^{[l-1]}}_{k=1} |w^{[l]}_{i,\\,k}| $\n",
    "##### $ \\hspace{0.15cm} $ ② Back propagation\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\frac{\\partial{}J}{\\partial{}W^{[l]}} = \\frac{1}{m} \\frac{\\partial{}J}{\\partial{}Z^{[l]}} (A^{[l-1]})^{T} + \\frac{\\lambda{}}{m} \\text{sign}(W^{[l]}) \\;\\; \\text{ where } \\; \\text{sign}(w^{[l]}_{i,\\,k}) = \\begin{cases} 1 & w^{[l]}_{i,\\,k}>0 \\\\ 0 & w^{[l]}_{i,\\,k}=0 \\; \\text{(undefined)} \\\\ -1 & w^{[l]}_{i,\\,k}<0 \\end{cases} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 효과** : 가중치의 크기에 상관없이 부호에 따라 일정한 상수값을 더하거나 빼는 효과\n",
    "#### $ = $ 가중치를 희소하게 만드는 효과 (many $ 0 $)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **릿지(Ridge;L2 Regularization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 비용 함수에 가중치 프로베니우스 놈(Frobenius;L2 norm)을 추가하는 방법\n",
    "##### $ \\hspace{0.15cm} $ ① Forward propagation(Cost function)\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} J(W^{[1]},\\, \\textbf{b}^{[1]},\\, \\cdots{},\\, W^{[L]},\\, \\textbf{b}^{[L]}) = \\frac{1}{m}\\sum^{m}_{i=1} \\ell{}^{(i)} + \\frac{\\lambda{}}{2m}\\sum^{L}_{l=1} || W^{[l]} ||_{F} $\n",
    "##### $ \\hspace{5.25cm} = \\frac{1}{m}\\sum^{m}_{i=1} \\ell{}^{(i)} + \\frac{\\lambda{}}{2m}\\sum^{L}_{l=1}\\sum^{i=n^{[l]}}_{i=1}\\sum^{n^{[l-1]}}_{k=1} (w^{[l]}_{i,\\,k})^{2} $\n",
    "##### $ \\hspace{0.15cm} $ ② Back-propagation\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\frac{\\partial{}J}{\\partial{}W^{[l]}} = \\frac{1}{m} \\frac{\\partial{}J}{\\partial{}Z^{[l]}} (A^{[l-1]})^{T} + \\frac{\\lambda{}}{m} W^{[l]} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 효과** : 가중치의 크기에 따라 더 크게(혹은 더 작게) 감소시키는 효과 $ = $ 가중치를 고르게 작게 유지하는 효과 (small all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **엘라스틱넷(ElasticNet;L1+L2 Regularization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 비용 함수에 L1 및 L2 정규화항을 추가하는 방법\n",
    "##### $ \\hspace{0.15cm} $ ① Forward propagation(Cost function)\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} J(W^{[1]},\\, \\textbf{b}^{[1]},\\, \\cdots{},\\, W^{[L]},\\, \\textbf{b}^{[L]}) = \\frac{1}{m}\\sum^{m}_{i=1} \\ell{}^{(i)} + \\frac{\\lambda{}_{1}}{m}\\sum^{L}_{l=1}\\sum^{i=n^{[l]}}_{i=1}\\sum^{n^{[l-1]}}_{k=1} |w^{[l]}_{i,\\,k}| + \\frac{\\lambda{}_{2}}{2m}\\sum^{L}_{l=1}\\sum^{i=n^{[l]}}_{i=1}\\sum^{n^{[l-1]}}_{k=1} (w^{[l]}_{i,\\,k})^{2} $\n",
    "##### $ \\hspace{0.15cm} $ ② Back propagation\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\frac{\\partial{}J}{\\partial{}W^{[l]}} = \\frac{1}{m} \\frac{\\partial{}J}{\\partial{}Z^{[l]}} (A^{[l-1]})^{T} + \\frac{\\lambda{}_{1}}{m} \\text{sign}(W^{[l]}) + \\frac{\\lambda{}_{2}}{m} W^{[l]} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **드롭 아웃(Drop Out)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델이 학습할 때 은닉층(hidden layer)의 각 노드에 대해 활성화/비활성화할지를 결정하는 베르누이 시행을 적용하는 방법 \n",
    "##### $ \\hspace{0.15cm} $ ① Forward propagation(Activate Function)\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\underset{n^{[l]}\\times{}m}{D^{[l]}} = \\begin{bmatrix} d^{[l](1)}_{1} & d^{[l](2)}_{1} & \\cdots{} & d^{[l](m)}_{1} \\\\ d^{[l](1)}_{2} & d^{[l](2)}_{2} & \\cdots{} & d^{[l](m)}_{2} \\\\ \\vdots{} & \\vdots{} & \\ddots{} & \\vdots{} \\\\ d^{[l](1)}_{n^{[l]}} & d^{[l](2)}_{n^{[l]}} & \\cdots{} & d^{[l](m)}_{n^{[l]}} \\end{bmatrix} \\;\\; \\text{ where } \\; d_{i,j} \\, \\sim{} \\text{Bernoulli}(p) $\n",
    "##### $ \\hspace{0.75cm} \\underset{n^{[l]}\\times{}m}{A^{[l]}} = \\frac{1}{p} \\cdot{} D^{[l]} \\odot{} h(Z^{[l]}) $\n",
    "##### $ \\hspace{0.15cm} $ ② Backward propagation\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\frac{\\partial{}J}{\\partial{}A^{[l]}} = \\frac{1}{p} \\cdot{} D^{[l]} \\odot{} (W^{[l+1]})^{T} \\cdot{} \\frac{\\partial{}J}{\\partial{}Z^{[l+1]}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)**\n",
    "##### $ \\hspace{0.15cm} \\text{if } \\; A^{[l]} = D^{[l]} \\odot{} h(Z^{[l]}). $\n",
    "##### $ \\hspace{0.3cm} \\mathbb{E}[A^{[l]}] = \\mathbb{E}[D^{[l]} \\odot{} h(Z^{[l]})] = \\mathbb{E}[D^{[l]}] \\odot{} \\mathbb{E}[h(Z^{[l]})] $\n",
    "##### $ \\hspace{1.325cm} = \\begin{bmatrix} p & p & \\cdots{} & p \\\\ \\vdots{} & \\vdots{} & \\ddots{} & \\vdots{} \\\\ p & p & \\cdots{} & p \\\\ \\end{bmatrix} \\odot{} \\mathbb{E}[h(Z^{[l]})] = p \\cdot{} \\mathbb{E}[h(Z^{[l]})] \\;\\; $ ($ \\because{} \\mathbb{E}[X] = p, \\;\\; X \\sim{} \\text{Bernoulli(p)} $)\n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 드롭아웃을 적용하면 활성화 출력의 기댓값이 기존 대비 $ p $만큼 작아지게 되어, 이를 보정처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 효과** : 특정 노드의 가중치 의존 제한(가중치 분산)하기 때문에 앙상블(ensemble) 효과 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 앙상블 효과 : 무작위로 선택된 노드의 부분 집합만이 활성화되면서, 여러 다른 모델처럼 작동하는 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **조기 종료(Early Stopping)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델의 훈련 성능이 더 이상 개선되지 않을 때 학습을 조기 종료(중단)하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 효과** : 모델의 가중치 과적합 제한"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 조기 종료는 비용 최적화와 정규화를 동시에 수행하기 때문에, 이를 독립적으로 처리하기는 힘듬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **데이터 증강(Data Argumentation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : **[CONTENTS]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
