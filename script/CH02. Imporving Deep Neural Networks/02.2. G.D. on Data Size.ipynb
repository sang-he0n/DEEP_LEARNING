{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.2. **Gradient Descent on Data Size**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **경사 하강법(Gradient Descent)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 비용 함수의 값을 최소화하기 위해 사용하는 최적화 알고리즘\n",
    "#### $ \\Rightarrow{} \\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\nabla{}_{\\theta{}_{t}}J \\;\\; \\text{ where } \\, t \\, \\text{ is learning step}. $\n",
    "#### **[GRAPH]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 데이터 사이즈 별 경사 하강법 종류** : \n",
    "##### $ \\hspace{0.15cm} $ ① 배치 경사하강법(Batch Gradient Descent)\n",
    "##### $ \\hspace{0.15cm} $ ② 미니배치 경사 하강법(Minibatch Gradient Descent)\n",
    "##### $ \\hspace{0.15cm} $ ③ 확률적 경사 하강법(Stochastic Gradient Descent;SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **배치 경사 하강법(Batch Gradient Descent)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 전체 데이터셋을 사용하여 비용 함수의 그래디언트를 계산하고, 이를 기반으로 파라미터를 한 번 업데이트하는 방법\n",
    "#### $ \\Rightarrow{} \\displaystyle\\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\frac{1}{m} \\sum^{m}_{i=1} \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}} \\;\\; \\text{ where } \\, \\nabla{}_{\\theta{}^{[l]}_{t}}J = \\frac{1}{m} \\sum^{m}_{i=1} \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 매 파라미터 업데이트 시 전체 데이터셋의 모든 샘플을 사용하여 그래디언트를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 그래디언트가 실제(모집단) 그래디언트와 가장 근접하며, 노이즈가 가장 작아 그래디언트 방향이 안정적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** \n",
    "##### $ \\hspace{0.15cm} $ 독립적인 개별 샘플에 대하여 계산되는 그래디언트의 기댓값이 모집단의 그래디언트와 동일하고, 분산이 유한하다고 가정할 때 \n",
    "##### $ \\hspace{0.15cm} \\text{if } \\, \\mathbb{E}[\\nabla{}_{\\theta{}^{[l]}} \\ell{}^{(i)}] = \\nabla{}_{\\theta{}^{[l]}}\\mathcal{J} \\;\\; \\text{ and } \\;\\; \\text{var}(\\nabla{}_{\\theta{}^{[l]}_{t}}\\ell{}^{(i)}) \\leq{} C \\;\\; \\text{ and } \\;\\; C \\, \\text{ is positive constant}. \\;\\; $ (독립사건 및 불편추정량 및 유한분산 가정)\n",
    "##### $ \\hspace{0.3cm} \\displaystyle\\text{var}(\\frac{1}{m} \\sum^{m}_{i=1} \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}}) = \\frac{m \\cdot{} \\text{var}(\\nabla{}_{\\theta{}^{[l]}_{t}}\\ell{}^{(i)})}{m^{2}} = \\frac{\\text{var}(\\nabla{}_{\\theta{}^{[l]}_{t}}\\ell{}^{(i)})}{m} \\leq{} \\frac{C}{m} $\n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 모든 샘플을 사용할 수록 샘플 그래디언트의 분산이 작아져 모집단 그래디언트와 가까워짐(노이즈가 작아지게 됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $ \\hspace{0.15cm} $ ② (손실함수가 $ \\, \\theta{}^{[l]}_{t} $ 에 대하여 볼록하다면) 수렴(최적값 도달)이 안정적임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① (미니배치 경사하강법에 비해) 전체 데이터셋 사이즈가 클 경우 메모리 부담이 큼\n",
    "##### $ \\hspace{0.15cm} $ ② 파라미터 업데이트에 긴 계산 시간 소요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **미니배치 경사 하강법(Minibatch Gradient Descent)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 전체 데이터셋을 일정 비율로 분할하여 그래디언트를 계산하고 파라미터를 업데이트하는 방법\n",
    "#### $ \\Rightarrow{} \\displaystyle\\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\frac{1}{|B|} \\sum_{i\\in{}B} \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}} \\;\\; \\text{ where } \\, \\nabla{}_{\\theta{}^{[l]}_{t}}J = \\frac{1}{|B|} \\sum_{i\\in{}B} \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}} $\n",
    "#### $ \\hspace{4.8cm} \\text{and } \\, B \\, \\text{ is mini batch index set}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 배치 크기를 조절하여 배치 경사 하강법과 SGD의 특징을 혼합하여 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 배치 단위로 벡터화된 연산이 가능하여 컴퓨팅 리소스를 효율적으로 활용\n",
    "##### $ \\hspace{0.15cm} $ ② (SGD에 비해) 샘플 그래디언트 분산이 작음\n",
    "##### $ \\hspace{0.15cm} $ ③ (손실함수가 $ \\, \\theta{}^{[l]}_{t} $ 에 대하여 비볼록하다면) 무작위성이 포함되어 수렴이 좀 더 유연해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① $ \\, |B| $ 는 $ \\, m $ 에 가까워질수록 배치 경사 하강법, $ \\, 1 $ 에 가까워질수록 SGD의 성질에 가까워지므로 적절하게 설정 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **확률적 경사 하강법(Stochastic Gradient Descent;SGD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 각 파라미터 업데이트 시 한 개의 데이터 포인트만을 사용하여 그래디언트를 계산하고 업데이트하는 방법\n",
    "#### $ \\Rightarrow{} \\displaystyle\\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}} \\;\\; \\text{ where } \\, \\nabla{}_{\\theta{}^{[l]}_{t}}J = \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 확률적 경사하강법은 미니배치 경사하강법의 특수한 경우로, 미니배치 크기를 $ \\, 1 $ 로 설정한 경우임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 매우 빈번한 파라미터 업데이트를 수행하며 랜덤성이 포함됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 파라미터 업데이트에 적은 계산 시간 소요\n",
    "##### $ \\hspace{0.15cm} $ ② 낮은 메모리 사용량\n",
    "##### $ \\hspace{0.15cm} $ ③ (손실 함수가 $ \\, \\theta{}^{[l]}_{t} $ 에 대하여 볼록함을 보장하지 못한다면) 지역 최소(local minima)에서 벗어날 가능성이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 샘플 그래디언트의 노이즈가 커 변동성이 크기에 최적해 근처에서 진동할 가능성이 큼\n",
    "##### $ \\hspace{0.15cm} $ ② (배치 경사 하강법에 비해) 손실 함수에 대한 파라미터의 곡률(curvature)이 큰 경우 최적화 과정에서 진동하게 되고, 반대의 경우 수렴 속도가 느려짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** \n",
    "##### $ \\hspace{0.15cm} \\text{if } \\, \\textbf{w} = \\begin{bmatrix} w_{1}, w_{2} \\end{bmatrix} \\;\\; \\text{ and } \\;\\; L(\\textbf{w}) = \\textbf{w}^{T}\\begin{bmatrix} a & 0 \\\\ 0 & b \\end{bmatrix} \\textbf{w} = \\textbf{w}^{T}A\\textbf{w} = aw^{2}_{1} + bw^{2}_{2} \\;\\; \\text{ and } \\;\\; b \\gg{} a > 0. $\n",
    "##### $ \\hspace{0.3cm} \\nabla_{\\textbf{w}} L(\\textbf{w}) = \\frac{\\partial{}}{\\partial{}\\textbf{w}}\\textbf{w}^{T}A\\textbf{w} = (A+A^{T})\\textbf{w} = 2A\\textbf{w} = \\begin{bmatrix} 2aw_{1} \\\\ 2bw_{2} \\end{bmatrix} $\n",
    "##### $ \\hspace{0.3cm} \\textbf{w}_{t+1} = \\textbf{w}_{t} - \\alpha{} \\nabla_{\\textbf{w}_{t}} L(\\textbf{w}) \\rightarrow{} \\begin{bmatrix} w_{1;t+1} \\\\ w_{2;t+1} \\end{bmatrix} = \\begin{bmatrix} w_{1;t} \\\\ w_{2;t} \\end{bmatrix} - \\alpha{} \\begin{bmatrix} 2aw_{1;t} \\\\ 2bw_{2;t} \\end{bmatrix} = \\begin{bmatrix} (1-2\\alpha{}a)w_{1;t} \\\\ (1-2\\alpha{}b)w_{2;t} \\end{bmatrix} $\n",
    "##### $ \\hspace{0.15cm} \\therefore{} $ 업데이트시 $ \\, b $ 가 무척 크기 때문에 $ \\, (1-2\\alpha{}b) $ 의 부호가 바뀌어 진동할 가능성이 커지고, $ \\, a $ 는 작기 때문에 $ \\, (1-2\\alpha{}a) $ 의 값은 작아 업데이트도 적게됨\n",
    "#### $ \\hspace{0.3cm} $ <img src=\"../../img/02.2. G.D. on Data Size (2).png\" width=\"50%\" height=\"50%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 이 때 $ \\, 2a $ 와 $ \\, 2b $ 는 손실함수 $ \\, L $ 은 개별 가중치의 곡률(가파름의 정도)이 됨\n",
    "##### $ \\Rightarrow{} H = \\begin{bmatrix} \\frac{\\partial{}L(\\textbf{w})}{\\partial{}w_{1}\\partial{}w_{1}} & \\frac{\\partial{}L(\\textbf{w})}{\\partial{}w_{1}\\partial{}w_{2}} \\\\ \\frac{\\partial{}L(\\textbf{w})}{\\partial{}w_{2}\\partial{}w_{1}} & \\frac{\\partial{}L(\\textbf{w})}{\\partial{}w_{2}\\partial{}w_{2}} \\end{bmatrix} = \\begin{bmatrix} 2a & 0 \\\\ 0 & 2b \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 상기 ②번의 단점은 업데이트하는 파라미터 벡터(혹은 행렬)에 모두 같은 학습률($ \\alpha{} $)을 적용해서 발생한 단점임"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTCH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
