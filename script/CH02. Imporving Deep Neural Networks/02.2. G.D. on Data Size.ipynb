{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH02.2. **Gradient Descent on Data Size**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **경사하강법(Gradient Descent)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : **[CONTENTS]**\n",
    "#### $ \\Rightarrow{} $ **[LATEX]**\n",
    "#### **[GRAPH]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** : \n",
    "##### $ \\hspace{0.15cm} $ ① 배치 경사하강법(Batch Gradient Descent)\n",
    "##### $ \\hspace{0.15cm} $ ② 미니배치 경사 하강법(Minibatch Gradient Descent)\n",
    "##### $ \\hspace{0.15cm} $ ③ 확률적 경사 하강법(Stochastic Gradient Descent;SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **배치 경사하강법(Batch Gradient Descent)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 전체 데이터셋을 사용하여 비용 함수의 그래디언트를 계산하고, 이를 기반으로 파라미터를 한 번 업데이트하는 방법\n",
    "#### $ \\Rightarrow{} \\displaystyle\\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\frac{1}{m} \\sum^{m}_{i=1} \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}} = \\theta{}^{[l]}_{t} - \\alpha{} \\frac{1}{m} \\sum^{m}_{i=1} \\nabla{}_{\\theta{}_{t}}\\ell{}^{(i)} \\;\\; \\text{ where } \\, t \\, \\text{ is learning step}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "##### $ \\hspace{0.15cm} $ ① **[CONTENTS]**\n",
    "##### $ \\hspace{0.15cm} $ ② **[CONTENTS]**\n",
    "##### $ \\hspace{0.15cm} $ ③ **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 정확한 그래디언트 계산\n",
    "##### $ \\hspace{0.15cm} $ ② 안정적인 수렴 \n",
    "##### $ \\hspace{0.15cm} $ ③ 재현성 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 전체 데이터셋 사이즈가 클 경우 메모리 부담이 큼\n",
    "##### $ \\hspace{0.15cm} $ ② 느린 파라미터 업데이트 속도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **미니배치 경사 하강법(Minibatch Gradient Descent)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 전체 데이터셋을 일정 비율로 분할하여 각 미니배치마다 그래디언트를 계산하고 파라미터를 업데이트하는 방법\n",
    "#### $ \\Rightarrow{} \\displaystyle\\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\frac{1}{|B|} \\sum_{i\\in{}B} \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}} \\;\\; \\text{ where } \\, B \\, \\text{ is mini batch index set}. $\n",
    "#### $ \\hspace{4.8cm} \\text{and } \\, \\sum_{i\\in{}B} \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}} \\, \\text{ is} \\textbf{ random variable}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "##### $ \\hspace{0.15cm} $ ① 배치 크기를 조절하여 배치 경사 하강법과 SGD의 _\n",
    "##### $ \\hspace{0.15cm} $ ② **[CONTENTS]**\n",
    "##### $ \\hspace{0.15cm} $ ③ **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 배치 단위로 벡터화된 연산이 가능하여 CPU/GPU를 효율적으로 활용\n",
    "##### $ \\hspace{0.15cm} $ ② (SGD에 비해) 변동성 감소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① $ \\, |B| $ 는 하이퍼파라미터이기에 경험적으로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **확률적 경사 하강법(Stochastic Gradient Descent;SGD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 전체 데이터셋을 하나의 데이터 포인트로 분할하여 각 미니배치마다 그래디언트를 계산하고 파라미터를 업데이트하는 방법\n",
    "#### $ \\Rightarrow{} \\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 확률적 경사하강법은 미니배치 경사하강법의 특수한 경우로, 미니배치 크기를 $ \\, 1 $ 로 설정한 경우임\n",
    "#### $ \\hspace{0.15cm} \\Rightarrow{} \\displaystyle\\theta{}^{[l]}_{t+1} = \\theta{}^{[l]}_{t} - \\alpha{} \\frac{1}{|B|} \\sum_{i\\in{}B} \\frac{\\partial{}\\ell{}^{(i)}}{\\partial{}\\theta{}^{[l]}_{t}} \\;\\; \\text{ where } \\, |B| = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "##### $ \\hspace{0.15cm} $ ① **[CONTENTS]**\n",
    "##### $ \\hspace{0.15cm} $ ② **[CONTENTS]**\n",
    "##### $ \\hspace{0.15cm} $ ③ **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 빠른 학습 속도\n",
    "##### $ \\hspace{0.15cm} $ ② 낮은 메모리 사용량\n",
    "##### $ \\hspace{0.15cm} $ ③ 전역 최적화 가능성 : 그래디언트의 노이즈로 인해 지역 최소값에서 벗어나 전역 최소값을 탐색할 가능성 높아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 업데이트 시 노이즈가 많아 최적해 근처에서 진동하기 떄문에 변동성 높음\n",
    "##### $ \\hspace{0.15cm} $ ② 수렴 속도 느림 : 최적해 근처에서 진동하여 정확한 수렴이 어려울 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
