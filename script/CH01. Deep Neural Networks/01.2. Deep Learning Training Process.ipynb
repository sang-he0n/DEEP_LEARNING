{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH01.2. **Deep Learning Training Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **딥러닝 학습 프로세스(Deep Learning Training Process)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델이 학습 데이터를 통해 네트워크의 파라미터(가중치, 편향)를 **반복적으로** 조정, 입력 데이터와 정답값 간의 차이를 최소화하는 과정\n",
    "#### **[GRAPH]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 순서** : \n",
    "##### $ \\hspace{0.15cm} $ ① 순전파(Forward Propagation) \n",
    "##### $ \\hspace{0.15cm} $ ② 비용 계산(Cost Computation) \n",
    "##### $ \\hspace{0.15cm} $ ③ 역전파(Backward Propagation)\n",
    "##### $ \\hspace{0.15cm} $ ④ 파라미터 최적화(Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **순전파(Forward Propagation;Phase)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 네트워크의 각 레이어 별로 가중치와 편향을 이용해 선형 및 활성화 변환을 거쳐 최종 출력(예측값)을 계산하는 단계\n",
    "#### $ \\Rightarrow{} \\hat{Y} = $ **[LATEX]**\n",
    "#### $ \\hspace{0.15cm} \\text{where } \\; Z^{[l]} = W^{[l]}A^{[l-1]}+B^{[l]} \\;  \\text{ and } \\; A^{[l]} = h^{[l]}(Z^{[l]}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 주어진 입력 데이터를 통해 네트워크가 최종 출력(예측값)을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **비용 계산(Cost Computation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 순전파 후 네트워크에서 출력한 예측값과 실제값 사이의 차이를 정의된 손실 함수를 통해 계산하는 단계\n",
    "#### $ \\Rightarrow{} J(W^{[1]},B^{[1]},\\cdots{}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 모델의 성능을 평가하고 학습의 방향성을 제시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **역전파(Backward Propagation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 비용을 각 파라미터로 편미분하여 모델의 각 파라미터가 비용에 미치는 영향(기울기;gradient)을 계산하는 단계\n",
    "#### $ \\Rightarrow{} \\frac{\\partial{}J}{\\partial{}\\theta{}^{[l]}} = \\frac{\\partial{}J}{\\partial{}A^{[L-1]}}\\frac{\\partial{}A^{[L-1]}}{\\partial{}Z^{[L_1]}} \\cdots{} \\frac{\\partial{}Z^{[l]}}{\\partial{}\\theta{}^{[l]}} \\;\\; \\text{ where } \\; \\theta{} \\, \\text{ is learnable parameter}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 손실값을 최소화하는 방향으로 각 파라미터를 얼마나 변화시켜야 할지 나타내는 기울기를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 수치적 미분(Numerical Differentiation)과 해석적 미분(Analytic Differentiation)** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 수치적 미분 : 아주 작은 변화량 $ \\epsilon{} $을 통해 함수의 변화량을 측정함으로써 미분을 근사적으로 구하는 방법\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\frac{\\partial{}f(x)}{\\partial{}x} \\approx{} \\frac{f(x+\\epsilon{})-f(x-\\epsilon{})}{2\\epsilon{}} \\;\\; $ (중심차분법)\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 해석적 미분 : 수식을 전개해($ = $ 미분의 기본 규칙을 적용해) 미분를 정확하게 구하는 방법\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\frac{\\partial{}f(x)}{\\partial{}x} = 4x \\;\\; \\text{ where } \\; f(x) = 2x^{2} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 실제 컴퓨팅 환경에서 딥러닝 모델은 계산 그래프를 구성한 뒤, 그 위에서 체인룰을 이용한 해석적 미분을 수행해 역전파를 시행함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 계산그래프(Computation Graph)** : (수학적) 연산 과정을 노드(node)와 간선(edge)로 연결해 나타낸 그래프 구조\n",
    "##### $ \\hspace{0.15cm} $ <img src=\"../../img/01.2. Deep Learning Training Process (1).png\" width=\"40%\" height=\"40%\"></img>\n",
    "##### $ \\hspace{0.15cm} \\frac{\\partial{}e}{\\partial{}a} = \\underbrace{{\\frac{\\partial{}e}{\\partial{}c}}}_{\\text{upstream grad.}} \\cdot{} \\underbrace{\\frac{\\partial{}c}{\\partial{}a}}_{\\text{local grad.}} = \\frac{\\partial{}}{\\partial{}c}(d*c) \\cdot{} \\frac{\\partial{}}{\\partial{}a} (a+b) = c * 1 = 2 * 1 = 2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **최적화(Optimization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 계산된 그래디언트의 경사 하강법을 이용하여 파라미터를 업데이트하여 손실을 최소화하는 단계\n",
    "#### $ \\Rightarrow{} \\theta{}^{[l]} \\coloneqq{} \\theta{}^{[l]} - \\eta{} \\frac{}{} (\\frac{\\partial{}J}{\\partial{}\\theta{}^{[l]}})^{T} = \\theta{}^{[l]} - \\eta{} \\nabla{}_{\\theta{}^{[l]}}J \\;\\; \\text{ where } \\; \\eta{} \\, \\text{ is learning-rate} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 손실을 최소화하는 방향으로 모델의 성능을 점진적 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
