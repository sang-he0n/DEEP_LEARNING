{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH01.2. **Deep Learning Training Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **딥러닝 학습 프로세스(Deep Learning Training Process)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델이 학습 데이터를 통해 네트워크의 파라미터(가중치, 편향)를 **반복적으로** 조정, 입력 데이터와 정답값 간의 차이를 최소화하는 과정\n",
    "#### **[GRAPH]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 순서** : \n",
    "##### $ \\hspace{0.15cm} $ ① 순전파(Forward Propagation) \n",
    "##### $ \\hspace{0.15cm} $ ② 손실 계산(Loss Computation) \n",
    "##### $ \\hspace{0.15cm} $ ③ 역전파(Backward Propagation)\n",
    "##### $ \\hspace{0.15cm} $ ④ 파라미터 최적화(Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **순전파(Forward Propagation;Phase)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 네트워크의 각 레이어 별로 가중치와 편향을 이용해 선형 및 활성화 변환을 거쳐 최종 출력(예측값)을 계산하는 단계\n",
    "#### $ \\Rightarrow{} \\hat{Y} = h^{[L]} \\circ{} Z^{[L]} \\circ{} h^{[L-1]} \\circ{} Z^{[L-1]} \\circ{} \\cdots{} = h^{[L]}(Z^{[L]}(h^{[L-1]}(Z^{[L-1]}(\\cdots{})))) $\n",
    "#### $ \\hspace{0.15cm} \\text{where } \\, Z^{[l]}(\\cdot{}) = W^{[l]}A^{[l-1]}+B^{[l]} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 주어진 입력 데이터를 통해 네트워크가 최종 출력(예측값)을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **손실 계산(Loss Computation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 순전파 후 네트워크에서 출력한 예측값과 실제값 사이의 차이를 정의된 손실함수를 통해 계산하는 단계\n",
    "#### $ \\Rightarrow{} L(Y, \\hat{Y}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 모델의 성능을 평가하고 학습의 방향성을 제시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **역전파(Backward Propagation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 손실을 각 파라미터로 편미분하여 모델의 각 파라미터가 손실에 미치는 영향(기울기;gradient)을 계산하는 단계\n",
    "#### $ \\Rightarrow{} \\frac{\\partial{}L}{\\partial{}\\theta{}^{[l]}} = \\frac{\\partial{}L}{\\partial{}A^{[L-1]}}\\frac{\\partial{}A^{[L-1]}}{\\partial{}Z^{[L_1]}} \\cdots{} \\frac{\\partial{}Z^{[l]}}{\\partial{}\\theta{}^{[l]}} \\;\\; \\text{ where } \\, \\theta{} \\, \\text{ is parameter(weight or bias)}. $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 손실값을 최소화하는 방향으로 각 파라미터를 얼마나 변화시켜야 할지 나타내는 기울기를 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 수치적 미분(numerical differentiation)과 해석적 미분(analytic differentiation)** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, $ 수치적 미분 : 아주 작은 변화량 $ \\, \\epsilon{} $ 을 통해 함수의 변화량을 측정함으로써 미분을 근사적으로 구하는 방법\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\frac{\\partial{}f(x)}{\\partial{}x} \\approx{} \\frac{f(x+\\epsilon{})-f(x-\\epsilon{})}{2\\epsilon{}} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, $ 해석적 미분 : 수식을 전개해($ = $ 미분의 기본 규칙을 적용해) 미분를 정확하게 구하는 방법\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\frac{\\partial{}f(x)}{\\partial{}x} = 4x \\;\\; \\text{ where } \\, f(x) = 2x^{2} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 실제 컴퓨팅에서 역전파는 체인룰을 이용한 해석적 미분을 통해 계산되고, 수치적미분은 계산 정확성(그라디언트 체크)을 위해 보조적으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **최적화(Optimization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 계산된 그래디언트를 이용하여 파라미터를 업데이트하여 손실을 최소화하는 단계\n",
    "#### $ \\Rightarrow{} \\theta{}^{[l]} \\coloneqq{} \\theta{}^{[l]} - \\alpha{} \\frac{}{} \\frac{\\partial{}L}{\\partial{}\\theta{}^{[l]}} \\;\\; \\text{ where } \\, \\alpha{} \\, \\text{ is learning-rate} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 손실을 최소화하는 방향으로 모델의 성능을 점진적 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
