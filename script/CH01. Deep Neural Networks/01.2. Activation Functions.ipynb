{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH01.2. **Activation Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **활성화 함수(Activation Function)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망의 노드 별 선형 출력($ Z $)을 특정 규칙에 따라 변환하여 출력하는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** : 비선형성을 적용해 복잡한 패턴을 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)** 비선형성을 적용하지 않는다면 레이어를 아무리 깊게 설정해도 모든 레이어별로 선형결합되어 단층 네트워크가 됨\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} W^{[l+1]}(W^{[l]}X) = \\tilde{W}X $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① 시그모이드(Sigmoid)\n",
    "##### $ \\hspace{0.15cm} $ ② 하이퍼볼릭 탄젠트(Hyperbolic Tangent)\n",
    "##### $ \\hspace{0.15cm} $ ③ 렐루(Rectified Linear Unit;ReLU)\n",
    "##### $ \\hspace{0.15cm} $ ④ 리키 렐루(Leaky Rectified Linear Unit;Leaky ReLU)\n",
    "##### $ \\hspace{0.15cm} $ ⑤ 엘루(Exponential Linear Unit;ELU)\n",
    "##### $ \\hspace{0.15cm} $ ⑥ 겔루(Gaussian Error Linear Unit;GeLU)\n",
    "##### $ \\hspace{0.15cm} $ ⑦ 실루(Sigmoid Linear Unit;Swish;SiLU)\n",
    "##### $ \\hspace{0.3cm} \\vdots{} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **시그모이드(Sigmoid)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력값을 $ 0 $과 $ 1 $ 사이의 값으로 출력하는 함수\n",
    "##### $ \\hspace{0.15cm} $ <img src=\"../../img/01.2. Activation Functions (1).png\" width=\"40%\" height=\"40%\"></img>\n",
    "##### $ \\hspace{0.15cm} $ ① 함수 : $ h(z) = \\sigma{}(z) = \\frac{e^{z}}{1+e^{z}}=\\frac{1}{1+e^{-z}} $\n",
    "##### $ \\hspace{0.15cm} $ ② 도함수 : $ h'(z) = \\frac{e^{-z}}{(1+e^{-z})^{2}} $\n",
    "##### $ \\hspace{2.42cm} = \\frac{1}{(1+e^{-z})} \\frac{e^{-z}}{(1+e^{-z})} = \\frac{1}{(1+e^{-z})} (1-\\frac{1}{(1+e^{-z})}) $\n",
    "##### $ \\hspace{2.42cm} = h(z) (1-h(z)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "##### $ \\hspace{0.15cm} $ ① 출력 범위 : $ (0, 1) $\n",
    "##### $ \\hspace{0.15cm} $ ② 출력 기댓값 : $ E[h(z)] > 0 \\;\\; $ (positive-centered)\n",
    "##### $ \\hspace{0.15cm} $ ③ 미분 가능성 : 전구간 미분 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 출력 범위가 $ 0 $과 $ 1 $ 사이기 때문에 확률적 해석이 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 입력값이 매우 크거나 작을 경우 도함수 값이 $ 0 $에 가까워져 깊은 신경망 학습 시 기울기 소실 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 기울기 소실(Gradient Vanishing)** : 역전파 과정에서 지역-미분이 $ 0 $에 가까워져 파라미터 업데이트가 제대로 되지 않는 현상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $ \\hspace{0.15cm} $ ② 출력 중심(평균)이 $ 0 $이 아니라 양수로 치우쳐져 있어, 그래디언트의 방향이 항상 한 방향으로 치우치는 그래디언트 편향 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **하이퍼볼릭 탄젠트(Hyperbolic Tangent)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력값을 $ -1 $과 $ 1 $ 사이의 값으로 출력하는 함수\n",
    "##### $ \\hspace{0.15cm} $ <img src=\"../../img/01.2. Activation Functions (2).png\" width=\"40%\" height=\"40%\"></img>\n",
    "##### $ \\hspace{0.15cm} $ ① 함수 : $ h(z) = \\text{tanh}(z) = \\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} $\n",
    "##### $ \\hspace{0.15cm} $ ② 도함수 : $ h'(z) = \\frac{(e^{z}-e^{-z})^{'}(e^{z}+e^{-z})-(e^{z}-e^{-z})(e^{z}+e^{-z})^{'}}{(e^{z}+e^{-z})^{2}} $\n",
    "##### $ \\hspace{2.42cm} = \\frac{(e^{z}+e^{-z})(e^{z}+e^{-z})-(e^{z}-e^{-z})(e^{z}-e^{-z})}{(e^{z}+e^{-z})^{2}} = \\frac{(e^{z}+e^{-z})^{2}-(e^{z}-e^{-z})^{2}}{(e^{z}+e^{-z})^{2}} $\n",
    "##### $ \\hspace{2.42cm} = \\frac{4}{(e^{z}+e^{-z})^{2}} $\n",
    "##### $ \\hspace{2.42cm} = \\frac{4}{4\\text{cosh}^{2}(z)} \\;\\; $ ($ \\because{} \\, \\text{cosh}(z) = \\frac{e^{z}+e^{-z}}{2} $)\n",
    "##### $ \\hspace{2.42cm} = \\frac{\\text{cosh}^{2}(z)-\\text{sinh}^{2}(z)}{\\text{cosh}^{2}(z)} \\;\\; $ ($ \\because{} \\, \\text{cosh}^{2}(z) - \\text{sinh}^{2}(z) = 1 $)\n",
    "##### $ \\hspace{2.42cm} = 1 - \\frac{\\text{sinh}^{2}(z)}{\\text{cosh}^{2}(z)} $\n",
    "##### $ \\hspace{2.42cm} = 1 - \\text{tanh}^{2}(z) $\n",
    "##### $ \\hspace{2.42cm} = 1 - h(z)^{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "##### $ \\hspace{0.15cm} $ ① 출력 범위 : $ (-1, 1) $\n",
    "##### $ \\hspace{0.15cm} $ ② 출력 기댓값 : $ E[h(z)] = 0 \\;\\; $ (zero-centered)\n",
    "##### $ \\hspace{0.15cm} $ ③ 미분 가능성 : 전구간 미분 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 출력이 $ 0 $을 중심으로 분포하므로 학습 시 기울기 방향이 균형 있게 전달됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 입력값이 크거나 작은 경우에는 기울기 소실 발생 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **렐루(Rectified Linear Unit;ReLU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력값이 $ 0 $보다 크면 그대로 출력하고, 그렇지 않다면 $ 0 $을 출력하는 함수\n",
    "##### $ \\hspace{0.15cm} $ <img src=\"../../img/01.2. Activation Functions (4).png\" width=\"40%\" height=\"40%\"></img>\n",
    "##### $ \\hspace{0.15cm} $ ① 함수 : $ h(z) = \\text{ReLU}(z) = \\text{max}(0,\\, z) = \\begin{cases} x & \\text{if } \\; x \\geq{} 0 \\\\ 0 & \\text{if } \\; x < 0 \\end{cases} $\n",
    "##### $ \\hspace{0.15cm} $ ② 도함수 : $ h'(z) = \\begin{cases} 1 & \\text{if } \\; z > 0 \\\\ \\text{not define} & \\text{if } \\; z = 0 \\\\ 0 & \\text{if } \\; z < 0 \\end{cases} $\n",
    "##### $ \\hspace{2.35cm} \\approx{} \\begin{cases} 1 & \\text{if } \\; z \\geq{} 0 \\\\ 0 & \\text{if } \\; z < 0 \\end{cases} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "##### $ \\hspace{0.15cm} $ ① 출력 범위 : $ [0, \\infty{}) $\n",
    "##### $ \\hspace{0.15cm} $ ② 출력 기댓값 : $ E[h(z)] > 0 \\;\\; $ (positive-centered)\n",
    "##### $ \\hspace{0.15cm} $ ③ 미분 가능성 : $ z = 0 $에서 미분 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 계산이 매우 간단하기에 연산 효율성이 높음\n",
    "##### $ \\hspace{0.15cm} $ ② 양의 입력에 대해 일정한 기울기를 제공하여, 역전파 시 기울기 소실 문제를 일부 완화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 음수 영역에서 미분값이 $ 0 $이므로, 많은 뉴런이 비활성화되어 학습이 멈출 위험 존재 (dying ReLU)\n",
    "##### $ \\hspace{0.15cm} $ ② 출력 평균이 양수에 편향되어 있어, 가중치 업데이트 시 그래디언트 편향 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **리키 렐루(Leaky Rectified Linear Unit;Leaky ReLU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력값이 음수일 때도 작은 기울기를 갖는 ReLU의 변형 함수\n",
    "##### $ \\hspace{0.15cm} $ <img src=\"../../img/01.2. Activation Functions (5).png\" width=\"40%\" height=\"40%\"></img>\n",
    "##### $ \\hspace{0.15cm} $ ① 함수 : $ h(z) = \\text{max}(\\alpha{} \\cdot{} z,\\, z) \\;\\; \\text{ where } \\; 0 \\leq{} \\alpha{} \\leq{} 1 $\n",
    "##### $ \\hspace{0.15cm} $ ② 도함수 : $ h'(z) = \\begin{cases} 1 & \\text{if } \\; z > 0 \\\\ \\text{not define} & \\text{if } \\; z = 0 \\\\ \\alpha{} & \\text{if } \\; z < 0 \\end{cases} $\n",
    "##### $ \\hspace{2.45cm} \\approx{} \\begin{cases} 1 & \\text{if } \\; z \\geq{} 0 \\\\ \\alpha{} & \\text{if } \\; z < 0 \\end{cases} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "##### $ \\hspace{0.15cm} $ ① 출력 범위 : $ (-\\infty{}, \\infty{}) $\n",
    "##### $ \\hspace{0.15cm} $ ② 출력 기댓값 : $ E[h(z)] > 0 \\;\\; $ (positive-centered)\n",
    "##### $ \\hspace{0.15cm} $ ③ 미분 가능성 : $ z = 0 $에서 미분 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "##### $ \\hspace{0.15cm} $ ① 양수 구간에서는 ReLU와 동일하게 작동하고, 음수 구간에서는 작은 기울기($ \\alpha{} $)를 유지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 음수 영역에서도 기울기를 제공하기에 뉴런이 완전히 죽는 문제를 완화 $ \\rightarrow{} $ (ReLU에 비해) 학습이 안정적으로 진행될 가능성이 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① $ \\alpha{} $는 하이퍼 파라미터이기에 경험적으로 설정해야함 (일반적으로 $ 0.001 \\sim{} 0.3 $)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **엘루(Exponential Linear Unit;ELU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 기존 렐루 함수의 음수 영역에서도 지수 함수를 사용하여 부드러운 출력\n",
    "##### $ \\hspace{0.15cm} $ <img src=\"../../img/01.2. Activation Functions (6).png\" width=\"40%\" height=\"40%\"></img>\n",
    "##### $ \\hspace{0.15cm} $ ① 함수 $ h(z) = \\begin{cases} z & \\text{if } \\; z > 0 \\\\ \\alpha{} \\cdot{} ( e^{z} - 1) & \\text{if } \\; z \\leq{} 0 \\end{cases} $\n",
    "##### $ \\hspace{0.15cm} $ ② 도함수 : $ h'(z) = \\begin{cases} 1 & \\text{if } \\; z > 0 \\\\ \\alpha{}e^{z} & \\text{if } \\; z \\leq{} 0 \\end{cases} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "##### $ \\hspace{0.15cm} $ ① 출력 범위 : $ (-\\alpha{}, \\infty{}) $\n",
    "##### $ \\hspace{0.15cm} $ ② 출력 기댓값 : $ E[h(z)] > 0 \\;\\; $ (positive-centered)\n",
    "##### $ \\hspace{0.15cm} $ ③ 미분 가능성 : 전구간 미분 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 출력이 $ 0 $을 중심으로 분포하므로 학습 시 기울기 방향이 균형 있게 전달됨\n",
    "##### $ \\hspace{0.15cm} $ ② 음수 영역에서도 미분값이 $ 0 $이 아니므로 기울기 소실 문제를 줄임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① $ \\alpha{} $는 하이퍼 파라미터이기에 경험적으로 설정해야함\n",
    "##### $ \\hspace{0.15cm} $ ② 지수 함수로 인한 오버헤드(overhead;계산시간이 추가로 더 걸리는 문제)가 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **겔루(Gaussian Error Linear Units;GELU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력값을 표준 정규분포 누적분포함수(CDF)로 확률적 게이팅하여 부드럽게 비선형 변환하는 함수\n",
    "#### $ = $ 입력값이 양수일 확률만큼 그 값을 스케일링하는 활성화 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 확률적 게이팅(Probalistic Gating)** : 입력값을 통과시킬지, 아니면 억제할지 확률로 결정하는 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $ \\hspace{0.15cm} $ <img src=\"../../img/01.2. Activation Functions (7).png\" width=\"40%\" height=\"40%\"></img>\n",
    "##### $ \\hspace{0.15cm} $ ① 함수 : $ h(z) = z \\cdot{} \\varPhi{} $\n",
    "##### $ \\hspace{2.05cm} \\approx{} \\frac{1}{2} z \\cdot{} \\Big[{} 1 + \\text{tanh}\\big({} \\sqrt{\\frac{2}{\\pi{}}}(z+0.044715z^{3}) \\big){} \\Big]{} $\n",
    "##### $ \\hspace{2.05cm} \\approx{} \\frac{1}{2} z \\cdot{} \\Big[{} 1 + \\text{tanh}\\big({} \\sqrt{\\frac{2}{\\pi{}}}(z+u) \\big){} \\Big]{} $\n",
    "##### $ \\hspace{1.225cm} \\text{where } \\; \\varPhi{} = \\int^{z}_{-\\infty{}} \\frac{1}{\\sqrt{2\\pi{}}}e^{-\\frac{k^{2}}{2}} \\text{dk}, \\;\\; u = 0.044715z^{3} $\n",
    "##### $ \\hspace{0.15cm} $ ② 도함수 : $ h'(z) \\approx{} \\frac{1}{2} \\Big[{} 1+\\text{tanh}(u) \\Big]{} + \\frac{1}{2} \\Big[{} 1+\\text{tanh}^{2}(u) \\Big]{} \\cdot{} u^{'} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "##### $ \\hspace{0.15cm} $ ① 출력 범위 : $ (-\\infty{}, \\infty{}) $\n",
    "##### $ \\hspace{0.15cm} $ ② 출력 기댓값 : $ E[h(z)] > 0 \\;\\; $ (positive-centered)\n",
    "##### $ \\hspace{0.15cm} $ ③ 미분 가능성 : 전구간 미분 가능 (근사식도 smooth함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① ReLU보다 부드럽고, $z<0$ 영역에서도 작은 음수 출력이 가능해 정보 손실 완화\n",
    "##### $ \\hspace{0.15cm} $ ② 입력값이 정규 분포이라면 수학적으로 자연스러운 활성화 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① $ \\varPhi(z) $ 혹은 tanh 근사 계산에 있어 ReLU보다 연산량 증가\n",
    "##### $ \\hspace{0.15cm} $ ② 단순한 형태가 아니라 직관적인 해석이 ReLU 대비 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **실루(Sigmoid Linear Unit;Swish;SiLU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력값에 시그모이드 함수를 곱해 크기에 따라 부드럽게 통과 비율을 조절하는 비선형 함수\n",
    "##### $ \\hspace{0.15cm} $ <img src=\"../../img/01.2. Activation Functions (8).png\" width=\"40%\" height=\"40%\"></img>\n",
    "##### $ \\hspace{0.15cm} $ ① 함수 : $ h(z) = z \\cdot{} \\sigma{}(\\beta{}z) $\n",
    "##### $ \\hspace{1.225cm} \\text{where } \\; \\beta{} \\, \\text{ is hyper-parameter} $\n",
    "##### $ \\hspace{0.15cm} $ ② 도함수 : $ h'(z) = \\sigma{}(\\beta{}z) + \\beta{}z \\cdot{} \\sigma{}(\\beta{}z) \\Big[{} 1 - \\sigma{}(\\beta{}z) \\Big]{} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** :\n",
    "##### $ \\hspace{0.15cm} $ ① 출력 범위 : $ (-\\infty{}, \\infty{}) $\n",
    "##### $ \\hspace{0.15cm} $ ② 출력 기댓값 : $ E[h(z)] > 0 \\;\\; $ (positive-centered)\n",
    "##### $ \\hspace{0.15cm} $ ③ 미분 가능성 : 전구간 미분 가능\n",
    "##### $ \\hspace{0.15cm} $ ④ $ \\beta{} $ 조정으로 함수 형태 변형 가능(작으면 선형, 크면 ReLU에 가까움)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 작은 음수 영역에서도 완전히 0이 되지 않아 정보 손실 완화\n",
    "##### $ \\hspace{0.15cm} $ ② 부드러운 곡선 덕분에 최적화 과정에서 gradient 흐름이 안정적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 시그모이드 연산이 포함돼 ReLU 대비 연산량 증가\n",
    "##### $ \\hspace{0.15cm} $ ② $ \\beta{} $는 하이퍼 파라미터이기에 경험적으로 설정해야함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
