{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH01.3. **Loss Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **손실 함수(Loss Function)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 모델의 예측값과 실제 정답 간의 차이를 측정 및 정량화한 지표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 종류** :\n",
    "##### $ \\hspace{0.15cm} $ ① 회귀(Regression) : MAE, MSE, $ \\, \\cdots{} $\n",
    "##### $ \\hspace{0.15cm} $ ② 분류(Classification) : 힌지 손실(Hinge Loss), 크로스 엔트로피(Cross Entropy), $ \\, \\cdots{} $ \n",
    "##### $ \\hspace{0.325cm} \\vdots{} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **평균 절대 오차(Mean Absolute Error;MAE;L1 Loss)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 오차를 절댓값으로 측정하는 회귀 지표\n",
    "#### $ \\Rightarrow{} \\ell{}(\\textbf{y}^{(i)},\\,\\hat{\\textbf{y}}^{(i)}) = |y^{(i)}-\\hat{y}^{(i)}| $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 오차가 라플라스 분포임을 가정할 때 최대 우도 추정(Maximum Likelihood Estimatation;MLE)으로 도출\n",
    "##### $ \\hspace{0.15cm} $ ② 오차의 크기가 선형적으로 증가 $ = $ 오차 크기만큼 손실이 증가\n",
    "##### $ \\hspace{0.15cm} $ ③ 전체 오차 분포의 중앙값(median)을 추정하려는 경향 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 절대 오차를 직관적으로 보여주기에 (MSE에 비해) 해석이 직관적임\n",
    "##### $ \\hspace{0.15cm} $ ② 실제 타겟($ y $)의 단위와 동일함\n",
    "##### $ \\hspace{0.15cm} $ ③ 이상치에 강건함(robust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 미분 불가능한 지점이 존재\n",
    "##### $ \\hspace{0.15cm} $ ② 도함수가 일정하기에 최적점에 더 빨리 수렴하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **평균 제곱 오차(Mean Squared Error;MSE;L2 Loss)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 오차를 제곱하여 측정한 회귀 지표\n",
    "#### $ \\Rightarrow{} \\ell{}(\\textbf{y}^{(i)},\\,\\hat{\\textbf{y}}^{(i)}) = (y^{(i)}-\\hat{y}^{(i)})^{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : \n",
    "##### $ \\hspace{0.15cm} $ ① 오차가 정규분포을 가정할 때 최대 우도 추정으로 도출\n",
    "##### $ \\hspace{0.15cm} $ ② 오차의 크기가 비선형적으로 증가 $ = $ 오차의 크기가 클수록 제곱되어 손실이 증가\n",
    "##### $ \\hspace{0.15cm} $ ③ 전체 오차 분포의 기댓값(mean)을 추정하려는 경향 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 모든 구간에서 미분 가능\n",
    "##### $ \\hspace{0.15cm} $ ② 도함수가 오차에 비례하기에 최적점 수렴 속도가 빠름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 단점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 실제 타겟($ y $)의 단위와 동일하지 않기에 해석이 직관적이지 않음\n",
    "##### $ \\hspace{0.15cm} $ ② 이상치에 민감해 하나의 큰 오차가 전체 비용(cost)을 지배할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **힌지 손실(Hinge Loss)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 마진(margin)을 이용하여 정답 클래스와 나머지 클래스 간 점수(score)의 차이를 측정하는 분류지표\n",
    "#### $ \\Rightarrow{} \\ell{}(\\textbf{y}^{(i)},\\,\\hat{\\textbf{y}}^{(i)}) = \\sum_{j\\neq{}y^{(i)}_{*}} \\max{}(0, z^{(i)}_{j} - z^{(i)}_{y^{(i)}_{*}}+\\Delta{}) \\;\\; \\text{ where } \\; \\Delta{} \\, \\text{ is margin} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 올바른 클래스의 점수(선형 출력;$ Z $)가 다른 클래스의 점수보다 최소한 마진만큼 높도록 설정함\n",
    "#### $ = $ 모델이 올바른 클래스를 충분히 높게 예측(마진 조건을 만족)하지 않을 때만 손실을 발생시켜 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① 명확한 마진을 고려하기 때문에 클래스 간의 구분을 뚜렷하게 만들어 일반화 성능 향상\n",
    "##### $ \\hspace{0.15cm} $ ② 이상치에 강건함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① (크로스 엔트로피에 비해) 확률적 해석이 어려움\n",
    "##### $ \\hspace{0.15cm} $ ② 일부 구간에서 미분 불가능하여 최적화가 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **크로스 엔트로피(Cross Entropy)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 정답 클래스와 예측 클래스 간의 차이(불일치 정도)를 확률로 측정하는 분류 지표\n",
    "#### $ \\Rightarrow{} \\ell{}(\\textbf{y}^{(i)},\\,\\hat{\\textbf{y}}^{(i)}) = -\\sum^{n_{y}}_{k=1} y^{(i)}_{k} \\log{}\\hat{y}^{(i)}_{k} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 특징** : 두 확률 분포 간의 차이를 엔트로피로 측정, 예측의 불확실성이 높을수록 손실 값이 커짐\n",
    "#### $ = $ 예측된 확률이 정답과 얼마나 차이나는지에 따라 항상 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 장점** : \n",
    "##### $ \\hspace{0.15cm} $ ① _\n",
    "##### $ \\hspace{0.15cm} $ ② 확률적 해석 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(4) 단점** :\n",
    "##### $ \\hspace{0.15cm} $ ① 모델이 극단적인 확률(예: $ 0.9999 $ vs $ 0.0001 $)을 예측할 때 손실이 매우 커지는 문제(over-shooting) 발생 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [][][][][]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 정보(Information)와 엔트로피(Entropy)** : \n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, $ 정보 : (정보이론에서) 어떠한 사건 $ A $ 의 **발생 빈도가 얼마나 드문지**를 정량적으로 나타낸 지표\n",
    "##### $ \\hspace{0.3cm} \\Rightarrow{} I(x) = - \\log{}P(A) = - \\log{}P(x) \\;\\; \\text{ where } \\; P(x) \\, \\text{ is probability of event } \\, A. $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, $ 엔트로피 : 확률분포 $ P(x) $ 에 대해 각 사건의 정보량에 해당 사건이 발생할 확률을 가중치로 하여 평균을 낸 값\n",
    "##### $ \\hspace{0.3cm} \\Rightarrow{} H(x) = -\\sum_{x} P(x)\\log{}P(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 크로스 엔트로피와 KL 발산(Kullback-Leibler divergence)의 관계 : \n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\,\\, D_{KL}(p|q) =\\sum_{x}p(x)\\log{}(\\frac{p(x)}{q(x)}) \\, $ : 두 확률 분포 $ p(x),\\, q(x) $ 의 확률적 차이를 측정한 지표\n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, $ 크로스 엔트로피는 $ p(x) $ 가 원핫벡터(분포)임을 가정할 때 KL 발산과 엔트로피로 분해됨\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} H(p,q) = D_{KL}(p|q) + H(p) = D_{KL}(p|q) -\\sum_{x} p(x)\\log{}p(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** 통계학적 관점의 크로스 엔트로피(cross entopy) : \n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, $ 크로스 엔트로피는 확률 분포 $ p(x) $에서 뽑힌 정답 데이터를, $ q(x) $를 모델로 표현할 때 필요한 평균 정보량을 측정함 (두 분포가 가까울수록 값이 작아짐)\n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, i $번 째 정답 벡터($ \\textbf{y}^{(i)} $)는 **실현화된** 확률 분포이며, 예측 벡터($ \\hat{\\textbf{y}}^{(i)} $) 역시 확률 변환 활성화함수(ex. 시그모이드, 소프트맥스)를 취하면 확률 분포가 됨 \n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} H(p, q) = -\\sum_{x} p(x) \\log{} q(x) = -\\sum^{n_{y}}_{k=1} y^{(i)}_{k} \\log{}\\hat{y}^{(i)}_{k} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, $ 최대우도추정의 관점에서는 카테고리컬 분포의 네거티브 로그 우도(negative log likelihood)임"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
