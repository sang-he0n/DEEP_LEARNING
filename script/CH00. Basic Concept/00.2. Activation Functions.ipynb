{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH00.2. **Activation Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Activation Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 신경망의 뉴런이 입력 신호의 가중합을 특정 규칙에 따라 변환하여 출력 신호를 생성하는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 역할** : 비선형성 도입을 통해 복잡한 패턴을 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3) 종류** :\n",
    "#### $ \\hspace{0.15cm} $ ① Sigmoid\n",
    "#### $ \\hspace{0.15cm} $ ② Hyperbolic Tangent(tanh)\n",
    "#### $ \\hspace{0.15cm} $ ③ Rectified Linear Unit(RELU)\n",
    "#### $ \\hspace{0.15cm} $ ④ Leaky Rectified Linear Unit(Leaky RELU)\n",
    "#### $ \\hspace{0.15cm} $ ⑤ Exponential Linear Unit(ELU)\n",
    "#### $ \\hspace{0.15cm} $ ⑥ Swish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Sigmoid**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력값을 $ \\, 0, \\, 1 \\, $ 사이의 값으로 변환하는 함수\n",
    "#### $ \\hspace{0.15cm} $ ① 함수 : $ \\, h(x) = \\sigma{}(x) = \\frac{1}{1+e^{-x}} $\n",
    "#### $ \\hspace{0.15cm} $ ② 도함수 : $ \\, h'(x) = h(x) (1-h(x)) $\n",
    "#### $ \\hspace{0.3cm} $ <img src=\"../../img/00.2. Activation Functions (1).png\" width=\"40%\" height=\"40%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** :\n",
    "#### $ \\hspace{0.15cm} $ ① 확률적 해석 : 이진 분류 문제에서 출력값을 확률로 해석\n",
    "#### $ \\hspace{0.15cm} $ ② **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`PLUS`)** 기울기 소실 발생 : 입력값이 너무 크거나 작으면 도함수 값이 $ \\, 0 $ 에 가까워져 학습이 제대로 이루어지지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Hyperbolic Tangent(tanh)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력값을 $ \\, -1, \\, 1 \\, $ 사이의 값으로 변환하는 함수\n",
    "#### $ \\hspace{0.15cm} $ ① 함수 : $ \\, h(x) = \\text{tanh}(x) = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} $\n",
    "#### $ \\hspace{0.15cm} $ ② 도함수 : $ \\, h'(x) = 1 - h(x)^{2} $\n",
    "#### $ \\hspace{0.3cm} $ <img src=\"../../img/00.2. Activation Functions (2).png\" width=\"40%\" height=\"40%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** :\n",
    "#### $ \\hspace{0.15cm} $ ① 출력 범위 확장 : (활성화) 출력값의 평균을 $ \\, 0 $ 에 가깝게 분포하게끔 만듬\n",
    "#### $ \\hspace{0.15cm} $ ② **[CONTENTS]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Rectified Linear Unit(RELU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력값을 $ \\, 0 $ 보다 크면 그대로 출력하고, $ \\, 0 $ 이하이면 $ \\, 0 $ 을 출력하는 함수\n",
    "#### $ \\hspace{0.15cm} $ ① 함수 : $ \\, h(x) = \\text{max}(0,\\, x) $\n",
    "#### $ \\hspace{0.15cm} $ ② 도함수 : $ \\, h'(x) = \\begin{cases} 1, \\;\\; x > 0 \\\\ 0, \\;\\; x \\leq{} 0 \\end{cases} $\n",
    "#### $ \\hspace{0.3cm} $ <img src=\"../../img/00.2. Activation Functions (3).png\" width=\"40%\" height=\"40%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** :\n",
    "#### $ \\hspace{0.15cm} $ ① 계산 효율성\n",
    "#### $ \\hspace{0.15cm} $ ② 기울기 소실 완화 : 음수가 아닌 영역에서 일정한 기울기를 유지하여 학습을 촉진 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(`PLUS`)** 입력값이 음수인 경우 순간기울기(미분값)는 $ \\, 0 $ 이 되어 학습되지 않는데, 이를 dying RELU 혹은 dead neurons라고 정의함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Leaky Rectified Linear Unit(Leaky RELU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 입력이 음수일 때도 작은 기울기를 갖는 RELU의 변형 함수\n",
    "#### $ \\hspace{0.15cm} $ ① 함수 : $ \\, h(x) = \\text{max}(\\alpha{} \\cdot{} x,\\, x) $\n",
    "#### $ \\hspace{0.15cm} $ ② 도함수 : $ \\, h'(x) = \\begin{cases} 1, \\;\\; x > 0 \\\\ \\alpha{}, \\;\\; x \\leq{} 0 \\end{cases} $\n",
    "#### $ \\hspace{0.3cm} $ <img src=\"../../img/00.2. Activation Functions (4).png\" width=\"40%\" height=\"40%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** :\n",
    "#### $ \\hspace{0.15cm} $ ① 죽은 뉴런 문제 완화 \n",
    "#### $ \\hspace{0.15cm} $ ② 기울기 소실 최소화 : 음수 영역에서도 기울기가 존재하여 정보 손실을 줄입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Exponential Linear Unit(ELU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : 기존 RELU 함수의 음수 영역에서도 지수 함수를 사용하여 부드러운 출력\n",
    "#### $ \\hspace{0.15cm} $ ① 함수 $ \\, h(x) = \\begin{cases} x, \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; x > 0 \\\\ \\alpha{} \\cdot{} ( e^{x} - 1), \\;\\; x \\leq{} 0 \\end{cases} $\n",
    "#### $ \\hspace{0.15cm} $ ② 도함수 : $ \\, h'(x) = \\begin{cases} 1, \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\; x > 0 \\\\ h(x) + \\alpha{}, \\;\\; x \\leq{} 0 \\end{cases} $ \n",
    "#### $ \\hspace{0.3cm} $ <img src=\"../../img/00.2. Activation Functions (5).png\" width=\"40%\" height=\"40%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** :\n",
    "#### $ \\hspace{0.15cm} $ ① 활성화 출력값($ A $)의 평균이 $ \\, 0 $ 근접 : 안정적으로 학습\n",
    "#### $ \\hspace{0.15cm} $ ② 비선형성 강화 : 음수 영역에서도 부드러운 비선형성을 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Swish**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1) 정의** : Sigmoid 함수와 입력값을 곱한 값을 출력하는 함수\n",
    "#### $ \\hspace{0.15cm} $ ① 함수 : $ \\, h(x) = x \\cdot{} \\sigma{}(\\beta{} \\cdot{} x) = x \\cdot{} \\frac{1}{1+e^{-\\beta{}\\cdot{}x}} $\n",
    "#### $ \\hspace{0.15cm} $ ② 도함수 : $ \\, h'(x) = f(x) + \\sigma{}(\\beta{} \\cdot{} x)(1 - \\beta{} \\cdot{} h(x)) = \\sigma{}(\\beta{} \\cdot{} x) + \\beta{} \\cdot{} x \\cdot{} \\sigma{}(\\beta{}\\cdot{}x)(1 - \\sigma{}(\\beta{} \\cdot{} x)) $\n",
    "#### $ \\hspace{0.3cm} $ <img src=\"../../img/00.2. Activation Functions (6).png\" width=\"40%\" height=\"40%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2) 목적** :\n",
    "#### $ \\hspace{0.15cm} $ ① 성능 향상 : 일부 작업에서 ReLU보다 더 나은 성능을 보임\n",
    "#### $ \\hspace{0.15cm} $ ② 비선형성 강화 : 부드러운 비선형성을 통해 신경망의 표현력을 높임"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATOGL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
